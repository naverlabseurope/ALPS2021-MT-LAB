{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Reference: https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf <br>\n",
    "Original Notebook: https://github.com/nyu-dl/AMMI-2019-NLP-Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install subword-nmt\n",
    "!pip install sacremoses\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install pandas\n",
    "!pip install sacrebleu\n",
    "!pip install matplotlib\n",
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Google Translate API for Comparison\n",
    "\n",
    "https://github.com/ssut/py-googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "google_translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To run this notebook in Google Colab, you need to the following first:\n",
    "1. Go to \"Runtime / Change runtime type\", then select \"GPU\" in the \"Hardware accelerator\" drop-down list\n",
    "2. Open this link: https://drive.google.com/drive/folders/1E07YaKths98YpoBCH2PjdtTPqOXgfdZB?usp=sharing\n",
    "3. Then go to \"Shared with me\" in your Google Drive, right-click the \"ALPS2022-NMT\" folder\n",
    "and select \"Add shortcut to Drive\"\n",
    "\"\"\"\n",
    "\n",
    "colab = False   # set to False to run locally and not from Google Colab\n",
    "\n",
    "if colab:\n",
    "    # Download the python files from the ALPS Github\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2021-MT-LAB/ALPS2022/data.py\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2021-MT-LAB/ALPS2022/models.py\n",
    "    # Mount your Google Drive, which should contain a link to \"ALPS2022-NMT\"\n",
    "    from google.colab import drive\n",
    "    drive.flush_and_unmount()\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '/content/drive/MyDrive/ALPS2022-NMT'   # TODO: upload new data and pre-trained models\n",
    "    !ls {root_dir}/*\n",
    "else:\n",
    "    # Download the datasets and pre-trained models\n",
    "    # Modify this script to download data in other language pairs than EN-FR\n",
    "    !scripts/download-data.sh\n",
    "    root_dir = '.'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import data\n",
    "import models\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "We will work with a small English to French dataset from https://www.manythings.org/anki/. It contains translations of short and simple sentences aimed at foreign language learners (from the [Tatoeba collaborative database](https://tatoeba.org/en/)). Of course, models trained on this data will not perform well on longer, more sophisticated sentences. They also won't be very robust to domain shift and input noise. To train stronger models, some larger datasets can be downloaded from https://www.statmt.org/wmt21/ or https://opus.nlpl.eu/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang, target_lang = 'en', 'fr'\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "pretrained_model_dir = os.path.join(root_dir, 'pretrained_models', f'{source_lang}-{target_lang}')\n",
    "model_dir = os.path.join('models', f'{source_lang}-{target_lang}')\n",
    "!mkdir -p {model_dir}\n",
    "!head -5 {data_dir}/train.en-fr.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data\n",
    "\n",
    "1. Load the BPE model\n",
    "2. Load the parallel corpora for this language pair (train, valid and test). `load_data` will load a corpus and tokenize it with the BPE model with the `preprocess` function.\n",
    "3. Create (or load) dictionaries that map BPE tokens to token IDs (`data.load_or_create_dictionary` function)\n",
    "4. Binarize the data: map source and target text sequences to sequences of IDs, and sort the training set by length (`data.binarize` function)\n",
    "5. Create batches (`data.BatchIterator` class): group multiple sequence pairs of similar length together, pad them to the maximum length and create numpy arrays that can be used to train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the BPE model (multilingual BPE model, works with French, German and English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = os.path.join(data_dir, 'bpecodes.de-en-fr')\n",
    "\n",
    "with open(bpe_path) as bpe_codes:\n",
    "    bpe_model = BPE(bpe_codes)\n",
    "\n",
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    return bpe_model.segment(line.lower())\n",
    "\n",
    "def postprocess(line):\n",
    "    return line.replace('@@ ', '')\n",
    "\n",
    "def load_data(source_lang, target_lang, split='train', max_size=None):\n",
    "    # max_size: max number of sentence pairs in the training corpus (None = all)\n",
    "    path = os.path.join(data_dir, f'{split}.{source_lang}-{target_lang}')\n",
    "    return data.load_dataset(path, source_lang, target_lang, preprocess=preprocess, max_size=max_size)   # set max_size to 10000 for fast debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load and preprocess the parallel corpora (these are pandas DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(source_lang, target_lang, 'train', max_size=None)   # set max_size to 10000 for fast debugging\n",
    "valid_data = load_data(source_lang, target_lang, 'valid')\n",
    "test_data = load_data(source_lang, target_lang, 'test')\n",
    "print(train_data.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load or create the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dict_path = os.path.join(pretrained_model_dir, f'dict.{source_lang}.txt')\n",
    "target_dict_path = os.path.join(pretrained_model_dir, f'dict.{target_lang}.txt')\n",
    "\n",
    "source_dict = data.load_or_create_dictionary(\n",
    "    source_dict_path,\n",
    "    train_data['source_tokenized'],\n",
    "    reset=False,    # set reset to True if you're changing the data or the preprocessing\n",
    ")\n",
    "print(source_dict.words[:100])\n",
    "\n",
    "target_dict = data.load_or_create_dictionary(\n",
    "    target_dict_path,\n",
    "    train_data['target_tokenized'],\n",
    "    reset=False,\n",
    ")\n",
    "print(target_dict.words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('source vocab size:', len(source_dict))\n",
    "print('target vocab size:', len(target_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use the dictionaries to map tokens to indices. The training set is also sorted by length for more efficient batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.binarize(train_data, source_dict, target_dict, sort=True)\n",
    "data.binarize(valid_data, source_dict, target_dict, sort=False)\n",
    "data.binarize(test_data, source_dict, target_dict, sort=False)\n",
    "print(train_data.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_size={}, valid_size={}, test_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n",
    "    len(train_data),\n",
    "    len(valid_data),\n",
    "    len(test_data),\n",
    "    train_data['source_len'].min(),\n",
    "    train_data['source_len'].max(),\n",
    "    train_data['source_len'].mean(),\n",
    "))\n",
    "\n",
    "print('Train source length distribution:')\n",
    "print(train_data['source_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build batches. The training batches are automatically shuffled before each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30       # maximum 30 tokens per sentence (longer sequences will be truncated)\n",
    "batch_size = 512   # maximum 512 tokens per batch (decrease if you get OOM errors, increase to speed up training)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = data.BatchIterator(train_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=True)\n",
    "valid_iterator = data.BatchIterator(valid_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)\n",
    "test_iterator = data.BatchIterator(test_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of training batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(train_iterator)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder-Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of usually two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence. Essentially, all we need is some mechanism to read the source sentence and create an encoding and some mechanism to read the encoding and decode it to the target language. \n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"I am not the\n",
    "black cat\" → \"Je ne suis pas le chat noir\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the meaning of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder is anything which takes in a sentence and gives us a representation for the sentence. \n",
    "\n",
    "The encoder of a seq2seq network can be a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "However, we will start with a simpler Bag-of-Words encoder and then move on to more complex encoders.\n",
    "\n",
    "### Bag-of-Words Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_encoder = models.BOW_Encoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    reduce='sum',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "--------------------\n",
    "\n",
    "The decoder is another network that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "### Decoder without Attention\n",
    "\n",
    "In the simplest seq2seq decoder we use only the last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector can be used as the initial hidden state for an RNN decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_decoder = models.RNN_Decoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = models.EncoderDecoder(\n",
    "    bow_encoder,\n",
    "    bow_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, *test_or_valid_iterators, record=False):\n",
    "    \"\"\"\n",
    "    model: instance of models.EncoderDecoder\n",
    "    test_or_valid_iterators: list of data.BatchIterator\n",
    "    record: save scores in the model checkpoint\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    # Compute chrF over all test or validation sets\n",
    "    for iterator in test_or_valid_iterators:\n",
    "        src, tgt = iterator.source_lang, iterator.target_lang\n",
    "        loss = 0\n",
    "        for batch in iterator:\n",
    "            loss += model.eval_step(batch) / len(iterator)\n",
    "        translation_output = model.translate(iterator, postprocess)\n",
    "        score = translation_output.score\n",
    "        output = translation_output.output\n",
    "\n",
    "        print(f'{src}-{tgt}: loss={loss:.2f}, chrF={score:.2f}')\n",
    "\n",
    "        if record:\n",
    "            model.record(f'{src}_{tgt}_loss', loss)\n",
    "            model.record(f'{src}_{tgt}_chrf', score)\n",
    "        \n",
    "        scores.append(score)\n",
    "\n",
    "    # Average the validation chrF scores\n",
    "    score = sum(scores) / len(scores)\n",
    "    if len(scores) > 1:\n",
    "        print(f'chrF={score:.2f}')\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def train_model(train_iterator, valid_iterators, model, checkpoint_path, epochs=10):\n",
    "    \"\"\"\n",
    "    train_iterator: instance of data.BatchIterator or data.MultiBatchIterator\n",
    "    valid_iterators: list of data.BatchIterator\n",
    "    model: instance of models.EncoderDecoder\n",
    "    checkpoint_path: path of the model checkpoint\n",
    "    epochs: iterate this many times over train_iterator\n",
    "    \"\"\"\n",
    "    epochs += model.epoch\n",
    "\n",
    "    reset_seed()\n",
    "\n",
    "    best_score = -1\n",
    "    for epoch in range(model.epoch + 1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print(f'Epoch [{epoch}/{epochs}]')\n",
    "\n",
    "        # Iterate over training batches for one epoch\n",
    "        with tqdm(enumerate(train_iterator), total=len(train_iterator)) as t:\n",
    "\n",
    "            for i, batch in t:\n",
    "                running_loss += model.train_step(batch)\n",
    "                t.postfix = f' loss={running_loss / (i + 1):.3f}'\n",
    "\n",
    "        # Average training loss for this epoch\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "\n",
    "        print(f'loss={epoch_loss:.3f}, time={time.time() - start:.2f}')\n",
    "        model.record('train_loss', epoch_loss)\n",
    "\n",
    "        score = evaluate_model(model, *valid_iterators, record=True)\n",
    "\n",
    "        # Update the model's learning rate based on current performance.\n",
    "        # This scheduler divides the learning rate by 10 if chrF does not improve.\n",
    "        model.scheduler_step(score)\n",
    "\n",
    "        # Save a model checkpoint if it has the best validation chrF so far\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            model.save(checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(f'Training completed. Best chrF is {best_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with BOW Encoder and RNN Decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'bow.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'bow.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    bow_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], bow_model,\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(bow_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Plot an encoder-decoder attention matrix\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    fig.colorbar(ax.matshow(attentions, cmap='bone', aspect='auto'))\n",
    "    xlabels = input_sentence.split() + [data.EOS_TOKEN]\n",
    "    ylabels = output_words.split() + [data.EOS_TOKEN]\n",
    "    ax.set_xticks(range(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    ax.set_yticks(range(len(ylabels)))\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def encode_as_batch(sentence, dictionary, source_lang, target_lang):\n",
    "    # Create a batch from a single sentence\n",
    "    sentence = f'{sentence} {data.EOS_TOKEN}'\n",
    "    tensor = dictionary.txt2vec(sentence).unsqueeze(0)\n",
    "    return {\n",
    "        'source': tensor,\n",
    "        'source_len': torch.from_numpy(np.array([tensor.shape[-1]])),\n",
    "        'source_lang': source_lang,\n",
    "        'target_lang': target_lang,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_translation(model, sentence, dictionary, source_lang, target_lang, return_output=False):\n",
    "    # Translate given sentence with given model. Also show translation outputs by Google Translate for comparison.\n",
    "    print('Source:', sentence)\n",
    "    sentence_tok = preprocess(sentence, is_source=True, source_lang=source_lang, target_lang=target_lang)\n",
    "    print('Tokenized source:', sentence_tok)\n",
    "    batch = encode_as_batch(sentence_tok, dictionary, source_lang, target_lang)\n",
    "    prediction, attn_matrix, enc_self_attn = model.decoding_step(batch)\n",
    "    prediction = prediction[0]\n",
    "    prediction_detok = postprocess(prediction)\n",
    "    print('Prediction:', prediction)\n",
    "    print('Detokenized prediction:', prediction_detok)\n",
    "\n",
    "    print('Google Translate ({}->{}): {}'.format(\n",
    "        source_lang,\n",
    "        target_lang,\n",
    "        google_translator.translate(sentence, src=source_lang, dest=target_lang).text,\n",
    "    ))\n",
    "    print('Google Translate on prediction ({}->{}): {}'.format(\n",
    "        target_lang,\n",
    "        source_lang,\n",
    "        google_translator.translate(prediction_detok, src=target_lang, dest=source_lang).text,\n",
    "    ))\n",
    "\n",
    "    results = {\n",
    "        'source': sentence,\n",
    "        'source_tokens': sentence_tok.split() + ['<eos>'],\n",
    "        'prediction_detok': prediction_detok,\n",
    "        'prediction_tokens': prediction.split(),\n",
    "    }\n",
    "\n",
    "    if attn_matrix is not None:\n",
    "        attn_matrix = attn_matrix[0].detach().cpu().numpy()\n",
    "        results['attention_matrix'] = attn_matrix\n",
    "        show_attention(sentence_tok, prediction, attn_matrix)\n",
    "    \n",
    "    if enc_self_attn is not None:\n",
    "        results['encoder_self_attention_list'] = enc_self_attn\n",
    "    \n",
    "    if return_output:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(bow_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest limitation of a Bag-of-Word encoder is that it is insensitive to word order: <br>\n",
    "when shuffling the words in the previous sentence, you get the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(bow_model, 'are hello ? how you', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(bow_model, \"she 's five years older than me .\", source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder + RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = models.RNN_Encoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_decoder = models.RNN_Decoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = models.EncoderDecoder(\n",
    "    rnn_encoder,\n",
    "    rnn_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with RNN Encoder and RNN Decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'rnn.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'rnn.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    rnn_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], rnn_model,\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(rnn_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to the BoW encoder, an RNN is sensitive to word ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'are hello ? how you', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, \"she 's five years older than me .\", source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder + RNN Decoder with Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_encoder = models.RNN_Encoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(rnn_attn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_decoder = models.AttentionDecoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_attn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_model = models.EncoderDecoder(\n",
    "    rnn_attn_encoder,\n",
    "    rnn_attn_decoder,\n",
    "    lr=0.0005,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with RNN Encoder and RNN Decoder with attention (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'rnn-attn.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'rnn-attn.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    rnn_attn_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], rnn_attn_model,\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(rnn_attn_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model and visualize attention matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, \"she 's five years older than me .\", source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "[Transformer](https://arxiv.org/abs/1706.03762) is currently the state-of-the-art for Machine Translation. The encoder uses self-attention over the previous layers. The decoder combines self-attention and encoder-decoder attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = models.TransformerEncoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    heads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder = models.TransformerDecoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    heads=4,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = models.EncoderDecoder(\n",
    "    transformer_encoder,\n",
    "    transformer_decoder,\n",
    "    lr=0.0005,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Transformer model (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'transformer.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'transformer.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    transformer_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], transformer_model,\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(transformer_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
    "    jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_head_view(results):\n",
    "    self_attention = results['encoder_self_attention_list']\n",
    "    tokens = results['source_tokens']\n",
    "    sentence_b_start = None\n",
    "    head_view(self_attention, tokens, sentence_b_start)\n",
    "\n",
    "def show_model_view(results):\n",
    "    self_attention = results['encoder_self_attention_list']\n",
    "    tokens = results['source_tokens']\n",
    "    sentence_b_start = None\n",
    "    model_view(self_attention, tokens, sentence_b_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_translation(transformer_model, 'hello how are you ?', source_dict, source_lang, target_lang, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_head_view(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_view(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_translation(transformer_model, \"she 's five years older than me .\", source_dict, source_lang, target_lang, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_translation(transformer_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang, return_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Transformer model\n",
    "\n",
    "Load a pre-trained **de, fr <-> en** model. The same dictionary and embeddings are shared between all languages, and language codes (`<lang:de>`, `<lang:en>`, `<lang:fr>`) are prepended to each source sequence to identify the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model_dir = os.path.join(root_dir, 'pretrained_models', 'de-en-fr')\n",
    "\n",
    "multi_dict = data.Dictionary.load(os.path.join(multi_model_dir, 'dict.txt'))\n",
    "\n",
    "encoder = models.TransformerEncoder(input_size=len(multi_dict), hidden_size=512, num_layers=2, heads=4)\n",
    "decoder = models.TransformerDecoder(output_size=len(multi_dict), hidden_size=512, num_layers=1, heads=4)\n",
    "decoder.embedding = encoder.embedding\n",
    "multi_model = models.EncoderDecoder(encoder, decoder, lr=0.0005, use_cuda=True, target_dict=multi_dict)\n",
    "\n",
    "checkpoint_path = os.path.join(multi_model_dir, 'transformer.pt')\n",
    "multi_model.load(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual evaluation\n",
    "\n",
    "Modify the `preprocess` function to automatically prepend language codes to all source sequences (when calling `get_translation`, or `load_data`).\n",
    "\n",
    "And load test sets in all language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    line = bpe_model.segment(line.lower())\n",
    "    if is_source:\n",
    "        line = f'<lang:{target_lang}> {line}'\n",
    "    return line\n",
    "\n",
    "test_iterators = []\n",
    "\n",
    "for pair in 'en-fr', 'fr-en', 'en-de', 'de-en', 'de-fr', 'fr-de':\n",
    "    src, tgt = pair.split('-')\n",
    "    dataset = load_data(src, tgt, 'test')\n",
    "    data.binarize(dataset, source_dict=multi_dict, target_dict=multi_dict, sort=False)\n",
    "    iterator = data.BatchIterator(dataset, src, tgt, batch_size=512, max_len=30, shuffle=False)\n",
    "    test_iterators.append(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(multi_model, *test_iterators[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_model, \"she 's five years older than me .\", multi_dict, source_lang='en', target_lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_model, 'sie ist fünf jahre älter als ich .', multi_dict, source_lang='de', target_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot translation\n",
    "\n",
    "In theory, the model can do **zero-shot** translation, i.e., translate between German and French even though it has never seen German-French sentence pairs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(multi_model, *test_iterators[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, in practice zero-shot performance is very bad. Interact with the model to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_model, 'sie ist fünf jahre älter als ich .', multi_dict, 'de', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_model, 'elle a cinq ans de plus que moi .', multi_dict, 'fr', 'de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "\n",
    "Choose one of these exercises, or both!\n",
    "\n",
    "### Hyper-parameter tuning\n",
    "\n",
    "Find the best hyper-parameters for Transformer **en-fr**. Share your best test chrF scores on Slack!\n",
    "\n",
    "*Don't forget to reload the `preprocess` function at the start of the notebook*\n",
    "\n",
    "- Hyper-parameters: `lr`, `batch_size`, `num_layers`, `hidden_size`, `dropout`, `heads`, etc.\n",
    "- Other improvements: modify the learning rate scheduler and optimizer in `models.EncoderDecoder`; use different embedding size and hidden size, etc.\n",
    "\n",
    "### Multilingual NMT\n",
    "\n",
    "Train your own multilingual NMT model.\n",
    "\n",
    "Tips:\n",
    "- Create a multilingual dictionary by concatenating the tokenized data in all languages. Or simply re-use the dictionary of the pre-trained model (`multi_dict`).\n",
    "- Use the same dictionary for the source and target sides, and share the embeddings between your encoder and decoder (do: `decoder.embedding = encoder.embedding`).\n",
    "- Use `data.MultiBatchIterator(iterator_list)` to concatenate a list of training iterators (one for each language pair) into a single iterator, which is compatible with `train_model`.\n",
    "- `train_model` can take a list of several validation iterators, which will let you validate your model on several language pairs.\n",
    "- Improve your model's performance on **de-fr** and **fr-de** by including training data for these languages pairs (`data/train.de-fr.de` and `data/train.de-fr.fr`).\n",
    "\n",
    "### Train bilingual or multilingual models on other language pairs\n",
    "\n",
    "- Modify and re-run `./download-data.sh` to download data in new languages, preprocess this data and train BPE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
