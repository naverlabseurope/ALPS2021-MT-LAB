{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Reference: https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf <br>\n",
    "Original Notebook: https://github.com/nyu-dl/AMMI-2019-NLP-Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch                  # to train neural networks\n",
    "!pip install subword-nmt            # for BPE tokenization\n",
    "!pip install sacremoses             # for word tokenization\n",
    "!pip install googletrans==3.1.0a0   # to use Google Translate\n",
    "!pip install pandas                 # to store datasets in memory\n",
    "!pip install sacrebleu              # for MT evaluation\n",
    "!pip install matplotlib             # for plotting\n",
    "!pip install requests               # to download stuff\n",
    "!pip install bertviz                # to visualize Transformer attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To run this notebook in Google Colab, you need to the following first:\n",
    "1. Go to \"Runtime / Change runtime type\", then select \"GPU\" in the \"Hardware accelerator\" drop-down list\n",
    "2. Open this link: https://drive.google.com/drive/folders/1E07YaKths98YpoBCH2PjdtTPqOXgfdZB?usp=sharing\n",
    "3. Then go to \"Shared with me\" in your Google Drive, right-click the \"ALPS2022-NMT\" folder\n",
    "and select \"Add shortcut to Drive\"\n",
    "\n",
    "Optionally, if you don't have a Google Drive account, you can set colab to False,\n",
    "and the data and models will be downloaded in Colab (might take longer).\n",
    "\"\"\"\n",
    "\n",
    "cpu = False    # set to True to run on CPU (much slower)\n",
    "colab = True   # set to False to run locally and not from Google Colab\n",
    "model_root = 'models'  # where new models will be saved\n",
    "\n",
    "if not os.path.exists('data.py'):\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2022-MT-LAB/main/data.py\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2022-MT-LAB/main/models.py\n",
    "    !mkdir -p scripts\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2022-MT-LAB/main/scripts/prepare.py -O scripts\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2022-MT-LAB/main/scripts/download-data.sh -O scripts\n",
    "        \n",
    "if colab:\n",
    "    # Download the python files from the ALPS Github\n",
    "    # Mount your Google Drive, which should contain a link to \"ALPS2022-NMT\"\n",
    "    from google.colab import drive\n",
    "    drive.flush_and_unmount()\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '/content/drive/MyDrive/ALPS2022-NMT'\n",
    "    # model_root = '/content/drive/MyDrive/ALPS2022-models' # uncomment to save your models to your Google Drive\n",
    "    !ls {root_dir}/*\n",
    "else:\n",
    "    # Download the datasets and pre-trained models\n",
    "    # Modify this script to download data in other language pairs than EN-FR\n",
    "    !scripts/download-data.sh\n",
    "    root_dir = '.'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import data\n",
    "import models\n",
    "import pandas\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "from data import load_dataset, binarize, load_or_create_dictionary, BatchIterator\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up Google Translate API for comparison\n",
    "from googletrans import Translator\n",
    "google_translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "We will work with a small English to French dataset from https://www.manythings.org/anki/. It contains translations of short and simple sentences aimed at foreign language learners (from the [Tatoeba collaborative database](https://tatoeba.org/en/)). Of course, models trained on this data will not perform well on longer, more sophisticated sentences. They also won't be very robust to domain shift and input noise. To train stronger models, some larger datasets can be downloaded from https://www.statmt.org/wmt21/ or https://opus.nlpl.eu/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify those to train models for a different language pair\n",
    "source_lang, target_lang = 'en', 'fr'\n",
    "\n",
    "# paths to the datasets and pretrained models\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "pretrained_model_dir = os.path.join(root_dir, 'pretrained_models', f'{source_lang}-{target_lang}')\n",
    "\n",
    "# path to the newly trained models\n",
    "model_dir = os.path.join(model_root, f'{source_lang}-{target_lang}')\n",
    "\n",
    "!mkdir -p {model_dir}\n",
    "!head -5 {data_dir}/train.en-fr.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data\n",
    "\n",
    "1. Load the BPE model\n",
    "2. Load the parallel corpora for this language pair (train, valid and test). `load_dataset` will load a corpus and tokenize it with the BPE model with the given `preprocess` function.\n",
    "3. Create (or load) dictionaries that map BPE tokens to token IDs (`load_or_create_dictionary` function)\n",
    "4. Binarize the data: map source and target text sequences to sequences of IDs, and sort the training set by length (`binarize` function)\n",
    "5. Create batches (`BatchIterator` class): group multiple sequence pairs of similar length together, pad them to the maximum length and create numpy arrays that can be used to train or evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed: initialize the random number generator for reproducibility\n",
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the BPE model (multilingual BPE model, works with French, German and English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_dir, f'train.{source_lang}-{target_lang}')\n",
    "valid_path = os.path.join(data_dir, f'valid.{source_lang}-{target_lang}')\n",
    "test_path = os.path.join(data_dir, f'test.{source_lang}-{target_lang}')\n",
    "bpe_path = os.path.join(data_dir, 'bpecodes.de-en-fr')\n",
    "\n",
    "with open(bpe_path) as bpe_codes:\n",
    "    bpe_model = BPE(bpe_codes)\n",
    "\n",
    "def preprocess(source_line, target_line=None, source_lang=None, target_lang=None):\n",
    "    # BPE segmentation: e.g., 'He overslept this morning .' -> he over@@ slept this morning .'\n",
    "    # modify this function to tweak the pre-processing (e.g., to add control tags / language codes).\n",
    "    # 'source_lang' and 'target_lang' are not used here, but will be needed for multilingual translation later on.\n",
    "    # 'preprocess' can also be called to tokenize a single source sentence (instead of a sentence pair)\n",
    "    source_line = bpe_model.segment(source_line.lower())\n",
    "    if target_line is not None:\n",
    "        target_line = bpe_model.segment(target_line.lower())\n",
    "    return source_line, target_line\n",
    "\n",
    "def postprocess(line):\n",
    "    # Merge BPE-tokenized sequences back into sequences of words:\n",
    "    # \"ce matin , il s' est réve@@ illé trop tard .\" -> \"ce matin , il s' est réveillé trop tard .\"\n",
    "    # Used to post-process the model predictions into human readable text.\n",
    "    return line.replace('@@ ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and preprocess the parallel corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(train_path, source_lang, target_lang, preprocess, max_size=None)  # pandas.DataFrame\n",
    "# set max_size to 10000 for fast debugging\n",
    "valid_data = load_dataset(valid_path, source_lang, target_lang, preprocess)\n",
    "test_data = load_dataset(test_path, source_lang, target_lang, preprocess)\n",
    "print(train_data[:5])   # to see the first 5 rows of train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load or create the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dict_path = os.path.join(pretrained_model_dir, f'dict.{source_lang}.txt')\n",
    "target_dict_path = os.path.join(pretrained_model_dir, f'dict.{target_lang}.txt')\n",
    "\n",
    "source_dict = load_or_create_dictionary(\n",
    "    source_dict_path,\n",
    "    train_data['source_tokenized'],\n",
    "    reset=False,    # set reset to True if you're changing the data or the preprocessing\n",
    ")\n",
    "print(source_dict.words[:100])   # print the first 100 words in the source vocabulary\n",
    "\n",
    "target_dict = load_or_create_dictionary(\n",
    "    target_dict_path,\n",
    "    train_data['target_tokenized'],\n",
    "    reset=False,\n",
    ")\n",
    "print(target_dict.words[:100])\n",
    "\n",
    "print('source vocab size:', len(source_dict))\n",
    "print('target vocab size:', len(target_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use the dictionaries to map tokens to indices. The training set is also sorted by length for more efficient batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binarize(train_data, source_dict, target_dict, sort=True)\n",
    "binarize(valid_data, source_dict, target_dict, sort=False)\n",
    "binarize(test_data, source_dict, target_dict, sort=False)\n",
    "print(train_data[:5])  # print the first 5 rows of train_data\n",
    "# The 'source_bin' and 'target_bin' columns contain the sequences of indices\n",
    "# Indices of 1 correspond to the EOS token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_size={}, valid_size={}, test_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n",
    "    len(train_data),\n",
    "    len(valid_data),\n",
    "    len(test_data),\n",
    "    train_data['source_len'].min(),\n",
    "    train_data['source_len'].max(),\n",
    "    train_data['source_len'].mean(),\n",
    "))\n",
    "\n",
    "print('Train source length distribution:')\n",
    "# The 90th percentile indicates the point where 90% percent of the data have values lower than this number.\n",
    "# We see that 90% of training examples have 14 source words or less\n",
    "# and 99% of all training examples have 27 source words or less.\n",
    "print(train_data['source_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build batches. The training batches are automatically shuffled before each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30       # maximum 30 tokens per sentence (longer sequences will be truncated)\n",
    "batch_size = 512   # maximum 512 tokens per batch (decrease if you get OOM errors, increase to speed up training)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = BatchIterator(train_data, source_lang, target_lang, batch_size, max_len, shuffle=True)\n",
    "valid_iterator = BatchIterator(valid_data, source_lang, target_lang, batch_size, max_len, shuffle=False)\n",
    "test_iterator = BatchIterator(test_data, source_lang, target_lang, batch_size, max_len, shuffle=False)\n",
    "\n",
    "print('Example of training batch:')\n",
    "print(next(iter(train_iterator)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence models\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder-Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of usually two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence. Essentially, all we need is some mechanism to read the source sentence and create an encoding and some mechanism to read the encoding and decode it to the target language. \n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"I am not the\n",
    "black cat\" → \"Je ne suis pas le chat noir\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the meaning of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "The encoder is anything which takes in a sentence and gives us a representation for the sentence. \n",
    "\n",
    "The encoder of a seq2seq network can be a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "However, we will start with a simpler Bag-of-Words encoder and then move on to more complex encoders.\n",
    "\n",
    "### Bag-of-Words encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_encoder = models.BOW_Encoder(\n",
    "    source_dict=source_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    reduce='sum',\n",
    ")\n",
    "\n",
    "print(bow_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The decoder\n",
    "\n",
    "The decoder is another network that takes the encoder's output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "### Decoder without attention\n",
    "\n",
    "In the simplest seq2seq decoder we use only the last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and the encoder's context vector and it updates its internal state, which is then used to predict the next word. The initial input token is the start-of-sequence <SOS> token. The next inputs are the decoder's own predictions (at test time) or the ground-truth tokens (at train time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_decoder = models.RNN_Decoder(\n",
    "    target_dict=target_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(bow_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = models.EncoderDecoder(\n",
    "    bow_encoder,\n",
    "    bow_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=not cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "`train_model` trains a model for a given number of epochs. It will evaluate this model on the validation sets after each training epoch, and save a checkpoint if the model has improved.\n",
    "\n",
    "`evaluate_model` computes validation loss and chrF.\n",
    "\n",
    "chrF (https://aclanthology.org/W16-2341/) is a string-based metric, less known than BLEU, but which has been shown to outperform BLEU (i.e., to correlate better with human judgment). It also has the advantage that, because it is at the character-level, it does not rely on word tokenization and is more language-independent than BLEU.\n",
    "\n",
    "However, (hopefully) researchers will gradually move away from string-based metrics, to use the superior learned metrics (e.g., BARTScore: https://arxiv.org/abs/2106.11520).\n",
    "\n",
    "`plot_loss` plots the model's performance on the training and validation set (train loss and validation loss/chrF). It can be used to diagnose overfitting issues: if the training loss continues decreasing while the validation loss increases, this can mean that we are not doing enough regularization (e.g., `dropout`) or that the model is just too big for this tiny training corpus.\n",
    "\n",
    "On the other hand, if the training loss seems to stagnate, this can mean that we're doing too much regularization or not using the right learning rate schedule. The initial learning is either too large or too small, or a different scheduler should be used. By default, we're using ReduceLROnPlateau, which divides the learning rate by 10 when validation chrF hasn't improved (by at least a 0.5 margin) over the previous best. Depending on the model, this can be either too aggressive or not aggressive enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, *test_or_valid_iterators, record=False):\n",
    "    \"\"\"\n",
    "    Evaluate given models with given test or validation sets. This will compute both chrF and validation loss.\n",
    "    \n",
    "    model: instance of models.EncoderDecoder\n",
    "    test_or_valid_iterators: list of BatchIterator\n",
    "    record: save scores in the model checkpoint\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    # Compute chrF and valid loss over all test or validation sets\n",
    "    for iterator in test_or_valid_iterators:\n",
    "        src, tgt = iterator.source_lang, iterator.target_lang\n",
    "        loss = 0\n",
    "        for batch in iterator:\n",
    "            loss += model.eval_step(batch) / len(iterator)\n",
    "        translation_output = model.translate(iterator, postprocess)\n",
    "        score = translation_output.score\n",
    "\n",
    "        print(f'{src}-{tgt}: loss={loss:.2f}, chrF={score:.2f}')\n",
    "\n",
    "        if record:  # store the metrics in the model checkpoint\n",
    "            model.record(f'{src}_{tgt}_loss', loss)\n",
    "            model.record(f'{src}_{tgt}_chrf', score)\n",
    "        \n",
    "        scores.append(score)\n",
    "\n",
    "    # Average the validation chrF scores\n",
    "    score = sum(scores) / len(scores)\n",
    "    if len(scores) > 1:\n",
    "        print(f'chrF={score:.2f}')\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def train_model(model, train_iterator, valid_iterators, checkpoint_path, epochs=10):\n",
    "    \"\"\"\n",
    "    Train given model for given number of epochs.\n",
    "    The best performing checkpoint (according to average chrF on 'valid_iterators') will be saved\n",
    "    under 'checkpoint_path'.\n",
    "    \n",
    "    By default, the optimizer, epoch counter and learning rate scheduler are not reset.\n",
    "    This means that this function can be called several times:\n",
    "        train_model(epochs=2) is equivalent to train_model(epochs=1); train_model(epochs=1)\n",
    "    Call model.reset_optimizer() to reset the model to its initial optimization settings.\n",
    "    \n",
    "    model: instance of models.EncoderDecoder\n",
    "    train_iterator: instance of data.BatchIterator used for generating training batches\n",
    "    valid_iterators: list of BatchIterator used for evaluation\n",
    "    checkpoint_path: path where the model will be saved\n",
    "    epochs: iterate this many times over train_iterator\n",
    "    \"\"\"\n",
    "    epochs += model.epoch\n",
    "\n",
    "    reset_seed()\n",
    "\n",
    "    best_score = -1\n",
    "    for epoch in range(model.epoch + 1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print(f'Epoch [{epoch}/{epochs}]')\n",
    "\n",
    "        # Iterate over training batches for one epoch\n",
    "        with tqdm(enumerate(train_iterator), total=len(train_iterator)) as t:\n",
    "\n",
    "            for i, batch in t:\n",
    "                running_loss += model.train_step(batch)\n",
    "                t.postfix = f' loss={running_loss / (i + 1):.3f}'\n",
    "\n",
    "        # Average training loss for this epoch\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "\n",
    "        print(f'loss={epoch_loss:.3f}, time={time.time() - start:.2f}')\n",
    "        model.record('train_loss', epoch_loss)\n",
    "\n",
    "        score = evaluate_model(model, *valid_iterators, record=True)\n",
    "\n",
    "        # Update the model's learning rate based on current performance.\n",
    "        # This scheduler divides the learning rate by 10 if chrF does not improve.\n",
    "        model.scheduler_step(score)\n",
    "\n",
    "        # Save a model checkpoint if it has the best validation chrF so far\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            model.save(checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(f'Training completed. Best chrF is {best_score:.2f}')\n",
    "\n",
    "\n",
    "def plot_loss(model):\n",
    "    \"\"\"\n",
    "    Plot the training VS validation loss and chrf for the given model\n",
    "    (provided those metrics are stored in the checkpoint)\n",
    "    \"\"\"\n",
    "    metrics = model.metrics\n",
    "    epochs = sorted(metrics.keys())\n",
    "    train_loss = [metrics[epoch]['train_loss'] for epoch in epochs]\n",
    "    valid_loss = [\n",
    "        mean(v for k, v in metrics[epoch].items() if 'loss' in k and k != 'train_loss')\n",
    "        for epoch in epochs\n",
    "    ]\n",
    "    chrf = [\n",
    "        mean(v for k, v in metrics[epoch].items() if 'chrf' in k)\n",
    "        for epoch in epochs\n",
    "    ]\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(epochs, train_loss, linestyle='solid', label='Train loss')\n",
    "    ax1.plot(epochs, valid_loss, linestyle='dashdot', label='Valid loss')\n",
    "    ax2.plot(epochs, chrf, 'g--', label='Valid chrF')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('chrF')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with BOW encoder and RNN decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "# Note that you can load the pre-trained model, then re-run this cell with train_again=False to continue training it\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'bow.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'bow.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    bow_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(bow_model, train_iterator, [valid_iterator],\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)\n",
    "\n",
    "plot_loss(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(bow_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(input, output, attention_weights):\n",
    "    \"\"\"\n",
    "    Plot an encoder-decoder attention matrix\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    fig.colorbar(ax.matshow(attention_weights, cmap='bone', aspect='auto'))\n",
    "    xlabels = input.split() + [data.EOS_TOKEN]\n",
    "    ylabels = output.split() + [data.EOS_TOKEN]\n",
    "    ax.set_xticks(range(len(xlabels)))\n",
    "    ax.set_xticklabels(xlabels, rotation=90)\n",
    "    ax.set_yticks(range(len(ylabels)))\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def encode_as_batch(sentence, dictionary):\n",
    "    \"\"\"\n",
    "    Create a batch from a single sentence\n",
    "    \"\"\"\n",
    "    sentence = f'{sentence} {data.EOS_TOKEN}'\n",
    "    tensor = dictionary.txt2vec(sentence).unsqueeze(0)\n",
    "    return {\n",
    "        'source': tensor,\n",
    "        'source_len': torch.tensor(tensor.shape[-1:]),\n",
    "    }\n",
    "\n",
    "\n",
    "def translate(model, sentence, preprocess=preprocess, source_lang=source_lang, target_lang=target_lang,\n",
    "              return_output=False):\n",
    "    \"\"\"\n",
    "    Translate given sentence with given model. Also show translation outputs by Google Translate for comparison.\n",
    "\n",
    "    sentence (str): sentence to translate\n",
    "    preprocess: function used to tokenize the input sentence\n",
    "    source_lang (str): source language code (used for Google Translate and as a parameter to \"preprocess\")\n",
    "    target_lang (str): target language code (used for Google Translate and as a parameter to \"preprocess\")\n",
    "    return_output: if True, returns the translation and attention matrices. If False, just prints the translations.\n",
    "    \"\"\"\n",
    "    sentence_tok, _ = preprocess(sentence, target_line=None, source_lang=source_lang, target_lang=target_lang)\n",
    "    print('Tokenized source:', sentence_tok)\n",
    "    batch = encode_as_batch(sentence_tok, model.source_dict)\n",
    "    prediction, attn_matrix, enc_self_attn = model.decoding_step(batch)\n",
    "    prediction = prediction[0]\n",
    "    prediction_detok = postprocess(prediction)\n",
    "    print('Prediction:', prediction)\n",
    "    print('Detokenized prediction:', prediction_detok)\n",
    "\n",
    "    print('Google Translate ({}->{}): {}'.format(\n",
    "        source_lang,\n",
    "        target_lang,\n",
    "        google_translator.translate(sentence, src=source_lang, dest=target_lang).text,\n",
    "    ))\n",
    "    print('Google Translate on prediction ({}->{}): {}'.format(\n",
    "        target_lang,\n",
    "        source_lang,\n",
    "        google_translator.translate(prediction_detok, src=target_lang, dest=source_lang).text,\n",
    "    ))\n",
    "\n",
    "    results = {\n",
    "        'source_tokens': sentence_tok.split() + ['<eos>'],\n",
    "        'prediction_detok': prediction_detok,\n",
    "        'prediction_tokens': prediction.split(),\n",
    "    }\n",
    "\n",
    "    if attn_matrix is not None:\n",
    "        attn_matrix = attn_matrix[0].detach().cpu().numpy()\n",
    "        results['attention_matrix'] = attn_matrix\n",
    "        show_attention(sentence_tok, prediction, attn_matrix)\n",
    "    \n",
    "    if enc_self_attn is not None:\n",
    "        results['encoder_self_attention_list'] = enc_self_attn\n",
    "    \n",
    "    if return_output:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate some English sentence with the model\n",
    "translate(bow_model, 'hello how are you ?')\n",
    "# The Google Translate outputs are shown for reference to non-French speakers:\n",
    "# - The en->fr output is a high-quality translation of the input sentence\n",
    "# - The fr->en output is a translation back into English of our model's French translation (so that you can assess its quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest limitation of a Bag-of-Words encoder is that it is insensitive to word order: <br>\n",
    "when shuffling the words in the previous sentence, you get the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(bow_model, 'you are hello ? how')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(bow_model, \"she 's five years older than me .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN encoder + RNN decoder\n",
    "\n",
    "Now let's look at a more powerful model, which also uses an RNN for encoding the source sequence. Contrary to the Bag-of-Words encoder, it is sensitive to word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = models.RNN_Encoder(\n",
    "    source_dict=source_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(rnn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_decoder = models.RNN_Decoder(\n",
    "    target_dict=target_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(rnn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = models.EncoderDecoder(\n",
    "    rnn_encoder,\n",
    "    rnn_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=not cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with RNN encoder and RNN decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "# Note that you can load the pre-trained model, then re-run this cell with train_again=False to continue training it\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'rnn.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'rnn.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    rnn_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(rnn_model, train_iterator, [valid_iterator],\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)\n",
    "\n",
    "plot_loss(rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(rnn_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, 'hello how are you ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrary to the BoW encoder, an RNN is sensitive to word ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, 'you are hello ? how')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, \"she 's five years older than me .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, 'i know that the last thing you want to do is help me .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN encoder + RNN decoder with encoder-decoder attention\n",
    "\n",
    "The previous model is limited as it needs to encode the entire source sentence into a single fixed-size vector. In this exercise, this is not a big limitation as we're dealing with very short inputs, but in real world tasks you may need to encode much longer sequences (e.g., long sentences, full documents, speech inputs, images, etc.)\n",
    "\n",
    "We will now see a similar model, but which uses an attention mechanism to look at all the encoder states, not just the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_encoder = models.RNN_Encoder(\n",
    "    source_dict=source_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(rnn_attn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_decoder = models.AttentionDecoder(\n",
    "    target_dict=target_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(rnn_attn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_model = models.EncoderDecoder(\n",
    "    rnn_attn_encoder,\n",
    "    rnn_attn_decoder,\n",
    "    lr=0.0005,\n",
    "    use_cuda=not cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with RNN Encoder and RNN Decoder with attention (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "# Note that you can load the pre-trained model, then re-run this cell with train_again=False to continue training it\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'rnn-attn.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'rnn-attn.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    rnn_attn_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(rnn_attn_model, train_iterator, [valid_iterator],\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(rnn_attn_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model and visualize attention matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap below shows how much attention is given to each source position by decoder time step.\n",
    "We see for instance that when generating the word \"bonjour\", the decoder looks at encoder state of source word \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_attn_model, 'hello how are you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_attn_model, \"she 's five years older than me .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_attn_model, 'i know that the last thing you want to do is help me .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model\n",
    "\n",
    "[Transformer](https://arxiv.org/abs/1706.03762) is currently the state of the art for Machine Translation (and many other NLP tasks). The encoder uses self-attention over the previous layers. The decoder combines self-attention and encoder-decoder attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = models.TransformerEncoder(\n",
    "    source_dict=source_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    heads=4,\n",
    ")\n",
    "\n",
    "print(transformer_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder = models.TransformerDecoder(\n",
    "    target_dict=target_dict,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    heads=4,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(transformer_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = models.EncoderDecoder(\n",
    "    transformer_encoder,\n",
    "    transformer_decoder,\n",
    "    lr=0.0005,\n",
    "    use_cuda=not cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this notebook, we're using the same learning rate scheduler for all models:\n",
    "`torch.optim.lr_scheduler.ReduceLROnPlateau`, which reduces the learning rate when the validation score (chrF)\n",
    "does not increase enough.\n",
    "Feel free to experiment with other schedulers, using the `scheduler_fn` and `scheduler_args` parameters.\n",
    "\n",
    "\n",
    "For example:\n",
    "```\n",
    "transformer_model = models.EncoderDecoder(\n",
    "    transformer_encoder,\n",
    "    transformer_decoder,\n",
    "    lr=0.0005,\n",
    "    use_cuda=not cpu,\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ExponentialLR,\n",
    "    scheduler_args={'gamma': 0.5},\n",
    ")\n",
    "```\n",
    "\n",
    "Transformers are often trained with warmup: starting with a small learning rate, increasing it up to a maximum value for the first N steps, them slowly decreasing it. Such a scheduler is implemented as `models.WarmupLR`.\n",
    "\n",
    "Deeper models can also be trained (Transformer encoders and decoders are often at least 6 layers). Regularization (`dropout` parameter) might need to be modified accordingly to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Transformer model (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "# Note that you can load the pre-trained model, then re-run this cell with train_again=False to continue training it\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'transformer.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'transformer.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    transformer_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(transformer_model, train_iterator, [valid_iterator],\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(transformer_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertviz: tool for visualizing attention in the Transformer model\n",
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
    "    jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_head_view(results):\n",
    "    \"\"\"\n",
    "    Interactive visualization to see the attention at all heads and layers (4 heads per layer in the current setting)\n",
    "    \"\"\"\n",
    "    self_attention = results['encoder_self_attention_list']\n",
    "    tokens = results['source_tokens']\n",
    "    head_view(self_attention, tokens, None)\n",
    "\n",
    "def show_model_view(results):\n",
    "    \"\"\"\n",
    "    Shows attention per layer and per head in separate graphics\n",
    "    \"\"\"\n",
    "    self_attention = results['encoder_self_attention_list']\n",
    "    tokens = results['source_tokens']\n",
    "    model_view(self_attention, tokens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = translate(transformer_model, 'hello how are you ?', return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_head_view(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_view(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"she 's five years older than me .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, 'i know that the last thing you want to do is help me .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Politeness control\n",
    "\n",
    "Some aspects of generation can be controlled thanks to special tokens in the input. For instance multi-domain models can be trained and used using source-side domain tags (https://aclanthology.org/R17-1049).\n",
    "\n",
    "This work https://aclanthology.org/N16-1005/ used special tokens to control the politeness of the output.\n",
    "\n",
    "We will implement this approach for English-French translation, to control the use of \"tu\" VS \"vous\" pronouns, which are formal/informal translations of \"you\".\n",
    "\n",
    "We only need to partition the training data into formal VS informal splits, by looking for occurrences of \"tu\" and \"vous\". Then, add source-side control tags depending on the politeness level of the target, and train the model with this.\n",
    "At test time, we only need to put the right control tag and the model will know how to interpret it to pick the right level of politeness.\n",
    "\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "As we only rely on the \"politeness control token,\" it is necessary to prepare distinctive polite and non-polite training samples from the corpus.\n",
    "\n",
    "While a lot of different aspects of French grammar can be considered here, to start with, we pick sentences that contain \"tu\" and \"vous\" — both meaning \"you\"  in English — and label them as \"non-polite\" and \"polite,\" respectively.\n",
    "\n",
    "### Regular expressions\n",
    "\n",
    "To extract the sentences that contain the words \"tu\" or \"vous\", we can use the following regular expressions:\n",
    "```python\n",
    "r'(^|\\W)(vous)(\\W|$)'\n",
    "r'(^|\\W)(tu)(\\W|$)'\n",
    "```\n",
    "They match sentences that contain the corresponding words by making sure that each word is preceded and followed by a \"non-word\" character (e.g., whitespace or dash)\n",
    "\n",
    "For more information on regexes, you can check out the following resources:\n",
    "- https://www.regular-expressions.info/tutorial.html\n",
    "- https://docs.python.org/3/library/re.html\n",
    "- https://regex101.com/#python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_formal(line):\n",
    "    \"\"\"\n",
    "    Contains formal French translations of \"you\"\n",
    "    \"\"\"\n",
    "    # Modify this regex to match other formal pronouns (e.g., votre/vos)\n",
    "    regex = r'(^|\\W)(vous)(\\W|$)'\n",
    "    return bool(re.search(regex, line, re.IGNORECASE))\n",
    "\n",
    "def is_informal(line):\n",
    "    \"\"\"\n",
    "    Contains informal French translations of \"you\"\n",
    "    \"\"\"\n",
    "    # Modify this regex to match other informal pronouns (e.g., ton/ta/tes)\n",
    "    regex = r'(^|\\W)(tu)(\\W|$)'\n",
    "    return bool(re.search(regex, line, re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding politeness control tags\n",
    "\n",
    "When we identify sentences that are either polite or non-polite, we can attach corresponding control tags in front of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_formal(source_line, target_line=None, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Tokenizes the given line pair and prepends the <formal> source-side tag \n",
    "    \"\"\"\n",
    "    source_line, target_line = preprocess(source_line, target_line)\n",
    "    source_line = f'<formal> {source_line}'\n",
    "    return source_line, target_line\n",
    "\n",
    "def preprocess_informal(source_line, target_line=None, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Tokenizes the given line pair and prepends the <informal> source-side tag \n",
    "    \"\"\"\n",
    "    source_line, target_line = preprocess(source_line, target_line)\n",
    "    source_line = f'<informal> {source_line}'\n",
    "    return source_line, target_line\n",
    "\n",
    "def preprocess_formal_or_informal(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for politeness control:\n",
    "    - keep only line pairs whose target side has French formal or informal pronouns\n",
    "    - prepend politeness control tags to the source side\n",
    "    \"\"\"\n",
    "    if is_formal(target_line):\n",
    "        return preprocess_formal(source_line, target_line)\n",
    "    elif is_informal(target_line):\n",
    "        return preprocess_informal(source_line, target_line)\n",
    "    else:  # this line pair in neither formal nor informal\n",
    "        # This example will be filtered out by load_dataset (uncomment below to keep it, without a control tag):\n",
    "        # return preprocess(source_line, target_line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and loading the dataset\n",
    "\n",
    "Finally, we can filter and load the dataset by passing the `preprocess_formal_or_informal` function to `load_dataset`.\n",
    "This will keep only the line pairs that contain formal or informal pronouns and preprocess the sources to add control tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same dataset as before\n",
    "train_path = os.path.join(data_dir, 'train.en-fr')\n",
    "valid_path = os.path.join(data_dir, 'valid.en-fr')\n",
    "\n",
    "# But preprocess it to keep only line pairs that use tu/vous pronouns and to append control tags\n",
    "train_data = load_dataset(\n",
    "    train_path, 'en', 'fr',\n",
    "    preprocess=preprocess_formal_or_informal,\n",
    ")\n",
    "\n",
    "valid_data = load_dataset(\n",
    "    valid_path, 'en', 'fr',\n",
    "    preprocess=preprocess_formal_or_informal,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up for training\n",
    "\n",
    "As we are introducing new vocabularies (i.e., the control tokens), we need to add them to our pretrained model's existing vocabulary.\n",
    "\n",
    "Here, we replace the last two most infrequent tokens so that we do not need to resize the vocabulary and embeddings.\n",
    "\n",
    "Note that the replaced words will now be mapped to UNK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dict = transformer_model.source_dict\n",
    "\n",
    "# Replace some infrequent tokens with the new control tokens (these words will now be mapped to UNK)\n",
    "# This is a bit dirty, but this way we don't have to resize the pretrained model's vocabulary and embeddings\n",
    "source_dict[len(source_dict) - 2] = '<formal>'\n",
    "source_dict[len(source_dict) - 1] = '<informal>'\n",
    "\n",
    "# Binarize the training and validation data with these vocabularies\n",
    "binarize(train_data, source_dict, target_dict, sort=True)\n",
    "binarize(valid_data, source_dict, target_dict, sort=False)\n",
    "\n",
    "# You can see that the training source examples now start with special tokens.\n",
    "print(train_data[:5])\n",
    "\n",
    "print('train_size={}, valid_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n",
    "    len(train_data),\n",
    "    len(valid_data),\n",
    "    train_data['source_len'].min(),\n",
    "    train_data['source_len'].max(),\n",
    "    train_data['source_len'].mean(),\n",
    "))\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = BatchIterator(train_data, 'en', 'fr', batch_size=512, max_len=30, shuffle=True)\n",
    "valid_iterator = BatchIterator(valid_data, 'en', 'fr', batch_size=512, max_len=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the EN-FR pretrained Transformer model with the new data\n",
    "new_checkpoint_path = os.path.join(model_root, 'en-fr', 'polite-transformer.pt')\n",
    "transformer_model.reset_optimizer()\n",
    "# Uncomment below to reload the pre-trained model\n",
    "# transformer_model.load(os.path.join(pretrained_model_dir, 'transformer.pt'), reset_optimizer=True)\n",
    "train_model(transformer_model, train_iterator, [valid_iterator], new_checkpoint_path, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"would you lend me your bicycle ?\", preprocess_formal, 'en', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"would you lend me your bicycle ?\", preprocess_informal, 'en', 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Can you improve the `is_formal` and `is_informal` functions to find more training examples?\n",
    "For instance, French possessives (ton/ta/test, votre/vos) also have this formality distinction.\n",
    "\n",
    "By default, `preprocess_formal_or_informal` will exclude any training example that is neither formal nor informal. This results in a very small and biased dataset. The resulting model will also catastrophically forget how to translate sentences that do not start with politeness tags. It may be beneficial (to avoid overfitting and catastrophic forgetting) to also include regular training examples, without any politeness tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling the gender of the output\n",
    "\n",
    "One known issue of machine translation models (and other NLP models) is that they tend to exhibit gender biases, caused by the same biases appearing in the training data. For instance, in case of ambiguity, a doctor is more likely to be translated as masculine and a nurse as feminine.\n",
    "\n",
    "For instance \"Dr. Dupont is very skilled\" -> \"Le Dr. Dupont est très compétent\" (\"compétent\" is masculine, the feminine form is \"compétente\").\n",
    "\n",
    "You will now use control tags to control the gender of the translation. Sentences starting with `<feminine>` will be translated with the feminine pronoun \"elle\" and translations of sentences starting with `<masculine>` will use the masculine pronoun \"il\".\n",
    "\n",
    "Unfortunately, we don't have a mainstream gender-neutral pronoun in French (like \"they\" in English).\n",
    "An option called \"inclusive writing\" consists in writing both pronouns (e.g., \"il/elle\"), but there aren't\n",
    "many natural occurrences of this in existing NLP datasets yet, so for simplicity we will stick to binary masculine/feminine.\n",
    "\n",
    "You can mostly mirror the \"Politeness control\" task and change the regular expressions.\n",
    "\n",
    "A notable difference with the previous task is that we now want to impose some feature in the output, that may be different to what appears in the input. For instance, `<feminine> he eats apples` should translate as `il mange des pommes`. Because such things rarely occur naturally in MT data (contrary to politeness ambiguities), we will need to do some data augmentation. This can be achieved by randomly swapping the masculine or feminine in English sources. Modify the `feminize` and `masculinize` functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_feminine(line):\n",
    "    \"\"\"\n",
    "    Contains the French feminine pronoun \"elle\"\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def is_masculine(line):\n",
    "    \"\"\"\n",
    "    Contains the French masculine pronoun \"il\"\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def preprocess_feminine(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for feminine line pairs: the source side will have a special <feminine> token\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def preprocess_masculine(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for masculine line pairs: the source side will have a special <masculine> token\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def feminize(line):\n",
    "    \"\"\"\n",
    "    Change the English pronouns in `line` to be feminine\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def masculinize(line):\n",
    "    \"\"\"\n",
    "    Change the English pronouns in `line` to be masculine\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def preprocess_masculine_or_feminine(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for gender control:\n",
    "    - add the <feminine> source tag to sentences pairs whose target side is feminine\n",
    "    - add the <masculine> source tag to sentences pairs whose target side is masculine\n",
    "    - do data augmentation to swap the source-side gender with probability 0.5\n",
    "    \"\"\"\n",
    "    if is_feminine(target_line):\n",
    "        if np.random.rand() < 0.5:\n",
    "            source_line = masculinize(source_line)\n",
    "        return preprocess_feminine(source_line, target_line)\n",
    "    elif is_masculine(target_line):\n",
    "        if np.random.rand() < 0.5:\n",
    "            source_line = feminize(source_line)\n",
    "        return preprocess_masculine(source_line, target_line)\n",
    "    else:\n",
    "        # return preprocess(source_line, target_line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the previous functions have been filled in, the following can be run without modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "\n",
    "train_data = load_dataset(train_path, 'en', 'fr', preprocess=preprocess_masculine_or_feminine)\n",
    "valid_data = load_dataset(valid_path, 'en', 'fr', preprocess=preprocess_masculine_or_feminine)\n",
    "\n",
    "source_dict = transformer_model.source_dict\n",
    "\n",
    "# Replace some infrequent tokens with the new control tokens (these words will now be mapped to UNK)\n",
    "# This is a bit dirty, but this way we don't have to resize the pretrained model's vocabulary and embeddings\n",
    "source_dict[len(source_dict) - 2] = '<feminine>'\n",
    "source_dict[len(source_dict) - 1] = '<masculine>'\n",
    "\n",
    "# Binarize the training and validation data with these vocabularies\n",
    "binarize(train_data, source_dict, target_dict, sort=True)\n",
    "binarize(valid_data, source_dict, target_dict, sort=False)\n",
    "\n",
    "# You can see that the training source examples now start with special tokens.\n",
    "print(train_data[:5])\n",
    "\n",
    "print('train_size={}, valid_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n",
    "    len(train_data),\n",
    "    len(valid_data),\n",
    "    train_data['source_len'].min(),\n",
    "    train_data['source_len'].max(),\n",
    "    train_data['source_len'].mean(),\n",
    "))\n",
    "\n",
    "train_iterator = BatchIterator(train_data, 'en', 'fr', batch_size=512, max_len=30, shuffle=True)\n",
    "valid_iterator = BatchIterator(valid_data, 'en', 'fr', batch_size=512, max_len=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the EN-FR pretrained Transformer model with the new data\n",
    "new_checkpoint_path = os.path.join(model_root, 'en-fr', 'gender-controllable-transformer.pt')\n",
    "transformer_model.reset_optimizer()\n",
    "# Uncomment below to reload the pre-trained model:\n",
    "# transformer_model.load(os.path.join(pretrained_model_dir, 'transformer.pt'), reset_optimizer=True)\n",
    "train_model(transformer_model, train_iterator, [valid_iterator], new_checkpoint_path, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"she goes to the ALPS winter school .\", preprocess_masculine, 'en', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"she goes to the ALPS winter school .\", preprocess_feminine, 'en', 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Translation\n",
    "\n",
    "We will now look at multilingual translation, another trendy topic in MT. A single model can be trained to translate from multiple languages into multiple languages (https://aclanthology.org/Q17-1024/, https://arxiv.org/abs/2010.11125).\n",
    "This is done by having a single multilingual BPE model and dictionary, shared between all languages. The embedding matrix (and other model parameters) are also shared across languages. And this multilingual model is trained on multiple parallel datasets (e.g., en->fr, fr->en, de->en, en->de). Controlling the target language can be achieved by using special tokens, like for politeness control.\n",
    "\n",
    "Load a pre-trained **de, fr <-> en** model. The multilingual dictionary includes tokens for all three languages plus the language codes (`<lang:de>`, `<lang:en>`, `<lang:fr>`), which are prepended to each source sequence to identify the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model_dir = os.path.join(root_dir, 'pretrained_models', 'de-en-fr')\n",
    "\n",
    "multi_dict = data.Dictionary.load(os.path.join(multi_model_dir, 'dict.txt'))\n",
    "\n",
    "encoder = models.TransformerEncoder(source_dict=multi_dict, hidden_size=512, num_layers=2, heads=4)\n",
    "decoder = models.TransformerDecoder(\n",
    "    target_dict=multi_dict,\n",
    "    hidden_size=512, num_layers=1, heads=4,\n",
    "    embedding=encoder.embedding)  # tied embeddings (multilingual models usually have shared source/target embeddings)\n",
    "\n",
    "multi_model = models.EncoderDecoder(encoder, decoder, lr=0.0005, use_cuda=not cpu)\n",
    "\n",
    "checkpoint_path = os.path.join(multi_model_dir, 'transformer.pt')\n",
    "multi_model.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual evaluation\n",
    "\n",
    "Modify the `preprocess` function to automatically prepend language codes to all source sequences (when calling `translate`, or `load_data`).\n",
    "\n",
    "And load test sets in all language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_multi(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    source_line, target_line = preprocess(source_line, target_line)\n",
    "    source_line = f'<lang:{target_lang}> {source_line}'\n",
    "    return source_line, target_line\n",
    "\n",
    "test_sets = OrderedDict()\n",
    "\n",
    "for pair in 'en-fr', 'fr-en', 'en-de', 'de-en', 'de-fr', 'fr-de':\n",
    "    src, tgt = pair.split('-')\n",
    "    path = os.path.join(data_dir, f'test.{src}-{tgt}')\n",
    "    dataset = load_dataset(path, src, tgt, preprocess_multi)\n",
    "    binarize(dataset, source_dict=multi_dict, target_dict=multi_dict, sort=False)\n",
    "    iterator = BatchIterator(dataset, src, tgt, batch_size=512, max_len=30, shuffle=False)\n",
    "    test_sets[pair] = iterator\n",
    "    \n",
    "en_centric_test_sets = list(test_sets.values())[:4]\n",
    "non_en_centric_test_sets = list(test_sets.values())[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(multi_model, *en_centric_test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate accepts preprocess, source_lang and target_lang arguments\n",
    "translate(multi_model, \"she 's five years older than me .\", preprocess_multi, 'en', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(multi_model, 'sie ist fünf jahre älter als ich .', preprocess_multi, 'de', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot translation\n",
    "\n",
    "In theory, the model can do **zero-shot** translation, i.e., translate between German and French even though it has never seen German-French sentence pairs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(multi_model, *non_en_centric_test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, in practice zero-shot performance is very bad. Interact with the model to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(multi_model, 'sie ist fünf jahre älter als ich .', preprocess_multi, 'de', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(multi_model, 'elle a cinq ans de plus que moi .', preprocess_multi, 'fr', 'de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation to a new language pair\n",
    "\n",
    "Large-scale multilingual MT models are great as they can provide translations for multiple language pairs with just a single model.\n",
    "\n",
    "However, these models tend to be very large in model parameters and require heavy computational power to train.\n",
    "\n",
    "Therefore, when adding a new language pair, instead of re-training the model using the existing and newly added corpora from scratch, it would be more efficient to finetune the pretrained model only using the new dataset.\n",
    "\n",
    "## Naive finetuning of the model\n",
    "In the above \"Multilingual Translation\" section, we observed poor zero-shot MT performance for the \"DE, FR <-> EN\" model.\n",
    "\n",
    "We saw that, while it is possible to do \"DE <-> FR\" translation, as the model has never seen such bilingual data, the performance was rather poor.\n",
    "\n",
    "Suppose now we want to explicitly train the model to additionally support the \"DE -> FR\" translation.\n",
    "One way is to load the corresponding dataset and finetune the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DE-FR training data\n",
    "src, tgt = 'de', 'fr'\n",
    "\n",
    "train_path = os.path.join(data_dir, f'train.{src}-{tgt}')\n",
    "valid_path = os.path.join(data_dir, f'valid.{src}-{tgt}')\n",
    "\n",
    "# set max_size to 10000 for fast debugging\n",
    "train_data = load_dataset(train_path, src, tgt, preprocess_multi, max_size=None)\n",
    "valid_data = load_dataset(valid_path, src, tgt, preprocess_multi)\n",
    "\n",
    "binarize(train_data, source_dict=multi_dict, target_dict=multi_dict, sort=True)\n",
    "binarize(valid_data, source_dict=multi_dict, target_dict=multi_dict, sort=False)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = BatchIterator(train_data, src, tgt, batch_size=512, max_len=30, shuffle=True)\n",
    "valid_iterator = BatchIterator(valid_data, src, tgt, batch_size=512, max_len=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the entire model on EN-FR\n",
    "new_checkpoint_path = os.path.join(model_root, 'de-en-fr', 'finetuned-transformer.pt')\n",
    "train_model(multi_model, train_iterator, [valid_iterator], new_checkpoint_path, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catastrophic forgetting\n",
    "After the finetuning, we evaluate the model on FR-EN and DE-FR test sets.\n",
    "\n",
    "Unfortunately, this finetuning resulted in a drop in performance for the FR-EN translation.\n",
    "\n",
    "This phenomenon of model forgetting previously learned information upon learning new information is called \"catastrophic forgetting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate on FR-EN and DE-FR test sets. We see a drop in FR-EN performance (catastrophic forgetting)\n",
    "chrf = evaluate_model(multi_model, test_sets['fr-en'], test_sets['de-fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter modules\n",
    "An alternative to finetuning and an effective way of bypassing the problem of catastrophic forgetting is the usage of adapter modules (https://arxiv.org/abs/1902.00751).\n",
    "\n",
    "An adapter module is often a small feedforward network with a skip connection, inserted in each Transformer layer.\n",
    "\n",
    "The insertion of adapter modules incurs additional model parameters, but they are often kept small compared to the size of the original network.\n",
    "\n",
    "During adapter tuning, only the adapter modules are trained with the downstream task's data — in our case, the DE-FR data — while the rest of the model parameters are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AdapterTransformerDecoder, AdapterTransformerEncoderLayer\n",
    "\n",
    "class AdapterLayer(nn.Module):\n",
    "    # This class definition is just for show. Adapter layers are actually defined in models.py\n",
    "    # Same adapter architecture as in this paper: https://arxiv.org/abs/1909.08478\n",
    "    def __init__(self, input_dim, projection_dim):\n",
    "        \"\"\"\n",
    "        input_dim: Transformer model's hidden size\n",
    "        projection_dim: bottleneck dimension of the adapter (usually smaller than input_dim), can be tuned\n",
    "        to control the amount of new parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(input_dim, projection_dim)\n",
    "        self.up = nn.Linear(projection_dim, input_dim)\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        # initialize the adapter weights to small values, so that it computes the identity function\n",
    "        # (or close enough) at the beginning of training (i.e., it keeps the Transformer layer outputs mostly\n",
    "        # unchanged)\n",
    "        nn.init.uniform_(self.down.weight, -1e-6, 1e-6)\n",
    "        nn.init.uniform_(self.up.weight, -1e-6, 1e-6)\n",
    "        nn.init.zeros_(self.down.bias)\n",
    "        nn.init.zeros_(self.up.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer_norm(x)\n",
    "        # down projection to a bottleneck dimension\n",
    "        y = self.down(y)\n",
    "        # non-linearity\n",
    "        y = F.relu(y)\n",
    "        # up projection to the model's dimension\n",
    "        y = self.up(y)\n",
    "        # residual connection\n",
    "        return x + y\n",
    "\n",
    "class AdapterTransformerEncoder(models.TransformerEncoder):\n",
    "    def __init__(self, adapter_ids, projection_dim, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a Transformer Encoder with adapter layers (that will be plugged in after each Transformer layer)\n",
    "        adapter_ids: list of adapter names (e.g., ['de-fr', 'fr-de']), one set of adapters will be created for each\n",
    "        of those. Adapters can be activated/deactivated thanks to the \"select_adapters\" function.\n",
    "        projection_dim: bottleneck dimension of the adapters\n",
    "        \"\"\"\n",
    "        self.adapter_ids = adapter_ids\n",
    "        self.projection_dim = projection_dim\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for name, param in self.named_parameters():\n",
    "            if '.adapters.' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def select_adapter(self, id):\n",
    "        # Use this method to activate a specific set of adapters (e.g., 'de-fr')\n",
    "        # Set id=None to deactivate adapters (and use the initial Transformer model)\n",
    "        for layer in self.layers:\n",
    "            layer.adapter_id = id\n",
    "\n",
    "    def build_layer(self, layer_id):\n",
    "        # This method can be modified to add adapters only at some layers (e.g., first encoder layer)\n",
    "        # Use models.TransformerEncoderLayer instead for standard Trandformer layers\n",
    "        return AdapterTransformerEncoderLayer(\n",
    "            self.adapter_ids,\n",
    "            self.projection_dim,\n",
    "            self.hidden_size,\n",
    "            self.heads,\n",
    "            self.dropout\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AdapterTransformerEncoder(\n",
    "    source_dict=multi_dict,\n",
    "    adapter_ids=['de-fr'],   # you can create adapters for more than one language pair\n",
    "    projection_dim=64,       # bottleneck dimension of the adapters\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    ")\n",
    "decoder = AdapterTransformerDecoder(\n",
    "    target_dict=multi_dict,\n",
    "    adapter_ids=['de-fr'],\n",
    "    projection_dim=64,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    heads=4,\n",
    "    embedding=encoder.embedding,\n",
    ")\n",
    "\n",
    "adapter_model = models.EncoderDecoder(encoder, decoder, lr=0.0005, use_cuda=not cpu)\n",
    "\n",
    "pretrained_checkpoint_path = os.path.join(multi_model_dir, 'transformer.pt')\n",
    "# Load the pre-trained model's parameters, set strict to False for partial initialization without errors.\n",
    "# As the pre-trained checkpoint does not contain parameters for the adapter layers, those are initialized at random.\n",
    "# We also reset the optimizer because its parameters to not match anymore and the learning rate might be too small.\n",
    "adapter_model.load(pretrained_checkpoint_path, strict=False, reset_optimizer=True)\n",
    "\n",
    "new_checkpoint_path = os.path.join(model_root, 'de-en-fr', 'adapter-transformer.pt')\n",
    "\n",
    "# Show the number of trained parameters.\n",
    "# All Transformer parameters are frozen except the adapter parameters.\n",
    "total_params = 0\n",
    "trained_params = 0\n",
    "for name, param in adapter_model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trained_params += param.numel()\n",
    "print(f'Total parameters: {total_params}, trained parameters: {trained_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the DE-FR adapters and train them on the DE-FR data (the other parameters are frozen)\n",
    "# Note that you can do encoder.select_adapter(None) to train only decoder adapters\n",
    "encoder.select_adapter('de-fr')\n",
    "decoder.select_adapter('de-fr')\n",
    "train_model(adapter_model, train_iterator, [valid_iterator], new_checkpoint_path, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning on adapters for evaluation\n",
    "\n",
    "After adapter training, we can turn on the DE-FR adapters to do inference. The advantage over full finetuning, is that we can easily turn them off to translate in the other language pairs, and avoid the catastrophic forgetting issue.\n",
    "\n",
    "We can see that with just 200K new parameters (1.4% of the initial model's size) we can adapt to the DE-FR direction without hurting performance of the other language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the DE-FR adapters to translate in the DE-FR direction\n",
    "encoder.select_adapter('de-fr')\n",
    "decoder.select_adapter('de-fr')\n",
    "chrf = evaluate_model(adapter_model, test_sets['fr-en'], test_sets['de-fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate the adapters to use the initial model (e.g., to translate in the English-centric directions).\n",
    "encoder.select_adapter(None)\n",
    "decoder.select_adapter(None)\n",
    "chrf = evaluate_model(adapter_model, test_sets['fr-en'], test_sets['de-fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "1. Can you train adapters to support more language pairs? (e.g., FR-DE). You can download data and train BPE models for more languages by modifying and re-running `scripts/download-data.sh` (warning: avoid re-running it for `de` and `fr` as it will generate different test splits)\n",
    "2. Can you achieve politeness control or gender control with adapters instead of control tags?\n",
    "3. Another technique to add new language pairs is to re-train (or finetune) the entire model on the new language pair's data **plus** the original language pairs. Train your own {DE,FR,EN}->{DE,FR,EN} multilingual model. Tip: you can use `data.concatenate_datasets(dataset_list)` to concatenate multiple datasets (created by `load_dataset`) into a single one, or `data.MultilingualBatchIterator(iterator_list)` to merge several batch iterators (created by `BatchIterator`) into a single one. The first and second solutions will respectively result in heterogeneous and homogeneous batches (i.e., containing sentences pairs of multiple or a single language pair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out the other topics in MT!\n",
    "There are also many other interesting and important research topics in MT which are not covered in this lab session.\n",
    "\n",
    "Here are some of them:\n",
    "- **Unsupervised or low-resource MT**\n",
    "  - How can we leverage monolingual data when bilingual data is not available or extremely scarce?\n",
    "  - How can we improve the performance of low-resourced language pairs?\n",
    "- **Document-level context-aware MT**\n",
    "  - How can we effectively translate a text containing multiple sentences while keeping the translation coherent and faithful?\n",
    "- **Domain-adapted or personalized MT, continual learning for MT**\n",
    "  - How can we extend an existing model for new domains, language pairs, or simply new addition of data?\n",
    "- **Efficient MT**\n",
    "  - How can we train and serve MT models more efficiently (both in terms of memory usage and CPU/GPU computation)\n",
    "  \n",
    "If you are interested in finding out more about MT, you can check out the [WMT conference](https://www.statmt.org/wmt21/) that is held annually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
