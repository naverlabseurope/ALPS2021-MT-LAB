{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation Lab @ ALPS Winter School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Reference: https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf <br>\n",
    "Original Notebook: https://github.com/nyu-dl/AMMI-2019-NLP-Part2\n",
    "\n",
    "# Outline\n",
    "\n",
    "1. [Setup](#1.-Setup): install modules, download datasets and pre-trained models, load and preprocess corpora\n",
    "2. [Sequence-to-sequence models](#2.-Sequence-to-sequence-models): Bag-of-Words vs RNNs vs Transformers\n",
    "3. [Controlling generation with input tags](#3.-Controlling-generation-with-input-tags): politeness control and gender control\n",
    "4. [Multilingual translation](#4.-Multilingual-Translation): zero-shot translation and adaptation to a new language pair\n",
    "5. [NLLB-200: a massively multilingual model](#5.-NLLB-200:-a-massively-multilingual-MT-model): try NLLB-200 and fine-tune it for domain adaptation and noise robustness\n",
    "\n",
    "Parts 1 and 2 are pre-requisites to the other parts, but parts 3, 4 and 5 can be run independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch                  # to train neural networks\n",
    "!pip install sentencepiece          # for tokenization\n",
    "!pip install googletrans==3.1.0a0   # to use Google Translate\n",
    "!pip install pandas                 # to store datasets in memory\n",
    "!pip install sacrebleu              # for MT evaluation\n",
    "!pip install matplotlib             # for plotting\n",
    "!pip install requests               # to download stuff\n",
    "!pip install --upgrade gdown        # to download files from Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To run this notebook in Google Colab, you need to the following first:\n",
    "1. Go to \"Runtime / Change runtime type\", then select \"GPU\" in the \"Hardware accelerator\" drop-down list\n",
    "2. Open this link: https://drive.google.com/drive/folders/1E07YaKths98YpoBCH2PjdtTPqOXgfdZB?usp=sharing\n",
    "3. Then go to \"Shared with me\" in your Google Drive, right-click the \"ALPS2023-NMT\" folder\n",
    "and select \"Add shortcut to Drive\"\n",
    "\n",
    "Optionally, if you don't have a Google Drive account, you can set colab to False,\n",
    "and the data and models will be downloaded in Colab (might take longer).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "cpu = False            # set to True to run on CPU (much slower)\n",
    "colab = True           # set to False to run locally and not from Google Colab\n",
    "model_root = 'models'  # where new models will be saved\n",
    "\n",
    "if not os.path.exists('data.py'):\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2023-MT-LAB/main/data.py\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2023-MT-LAB/main/models.py\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2023-MT-LAB/main/utils.py\n",
    "    !mkdir -p scripts\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2023-MT-LAB/main/scripts/prepare.py -O scripts/prepare.py\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2023-MT-LAB/main/scripts/download-data.sh -O scripts/download-data.sh\n",
    "    !wget https://raw.githubusercontent.com/naverlabseurope/ALPS2023-MT-LAB/main/scripts/download-nllb.sh -O scripts/download-nllb.sh\n",
    "        \n",
    "if colab:\n",
    "    # Download the python files from the ALPS Github\n",
    "    # Mount your Google Drive, which should contain a link to \"ALPS2023-NMT\"\n",
    "    from google.colab import drive\n",
    "    drive.flush_and_unmount()\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '/content/drive/MyDrive/ALPS2023-NMT'\n",
    "    # model_root = '/content/drive/MyDrive/ALPS2023-models' # uncomment to save your models to your Google Drive\n",
    "    !ls {root_dir}/*\n",
    "else:\n",
    "    # Download the datasets and pre-trained models\n",
    "    # Modify this script to download data in other language pairs than EN-FR\n",
    "    !bash scripts/download-data.sh\n",
    "    root_dir = '.'\n",
    "\n",
    "import os, sys, re, time\n",
    "import sacrebleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import data, models, utils\n",
    "from data import load_dataset, binarize, load_or_create_dictionary, BatchIterator, Tokenizer\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up Google Translate API for comparison\n",
    "from googletrans import Translator\n",
    "google_translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "We will work with a small English to French dataset named Tatoeba. It contains translations of short and simple sentences aimed at foreign language learners (from the [Tatoeba collaborative database](https://tatoeba.org/en/)). Of course, models trained on this data will not perform well on longer, more sophisticated sentences. They also won't be very robust to domain shift and input noise. To train stronger models, some larger datasets can be downloaded from https://www.statmt.org/wmt22/ or https://opus.nlpl.eu/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modify those to train models for a different language pair\n",
    "source_lang, target_lang = 'en', 'fr'\n",
    "\n",
    "# paths to the datasets and pretrained models\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "pretrained_model_dir = os.path.join(root_dir, 'pretrained_models', f'{source_lang}-{target_lang}')\n",
    "\n",
    "# path to the newly trained models\n",
    "model_dir = os.path.join(model_root, f'{source_lang}-{target_lang}')\n",
    "\n",
    "!mkdir -p {model_dir}\n",
    "!head -5 {data_dir}/train.en-fr.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data\n",
    "\n",
    "1. Load the BPE tokenizer\n",
    "2. Load the parallel corpora for this language pair (train, valid and test). `load_dataset` will load a corpus and tokenize it with the BPE model with the given `preprocess` function.\n",
    "3. Create (or load) dictionaries that map BPE tokens to token IDs (`load_or_create_dictionary` function)\n",
    "4. Binarize the data: map source and target text sequences to sequences of IDs, and sort the training set by length (`binarize` function)\n",
    "5. Create batches (`BatchIterator` class): group multiple sequence pairs of similar length together, pad them to the maximum length and create numpy arrays that can be used to train or evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed: initialize the random number generator for reproducibility\n",
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the BPE tokenizer (multilingual: works with French, German and English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_dir, f'train.{source_lang}-{target_lang}')\n",
    "valid_path = os.path.join(data_dir, f'valid.{source_lang}-{target_lang}')\n",
    "test_path = os.path.join(data_dir, f'test.{source_lang}-{target_lang}')\n",
    "bpe_path = os.path.join(data_dir, 'spm.de-en-fr.model')\n",
    "\n",
    "tokenizer = Tokenizer(bpe_path)\n",
    "\n",
    "def preprocess(source_line, target_line=None, source_lang=None, target_lang=None):\n",
    "    # BPE segmentation: e.g., 'He overslept this morning .' -> '▁He ▁o vers le pt ▁this ▁morning .'\n",
    "    # modify this function to tweak the pre-processing (e.g., to add control tags / language codes).\n",
    "    # 'source_lang' and 'target_lang' are not used here, but will be needed for multilingual translation later on.\n",
    "    # 'preprocess' can also be called to tokenize a single source sentence (instead of a sentence pair)\n",
    "    source_line = tokenizer.tokenize(source_line)\n",
    "    target_line = tokenizer.tokenize(target_line)\n",
    "    return source_line, target_line\n",
    "\n",
    "def postprocess(line):\n",
    "    # Merge BPE-tokenized sequences back into sequences of words:\n",
    "    # \"▁Ce ▁matin , ▁il ▁s ' est ▁réve illé ▁trop ▁tard .\" -> \"Ce matin, il s'est réveillé trop tard.\"\n",
    "    # Used to post-process the model predictions into human-readable text.\n",
    "    return tokenizer.detokenize(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and preprocess the parallel corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(train_path, source_lang, target_lang, preprocess, max_size=None)  # pandas.DataFrame\n",
    "# set max_size to 10000 for fast debugging\n",
    "valid_data = load_dataset(valid_path, source_lang, target_lang, preprocess, max_size=500)\n",
    "test_data = load_dataset(test_path, source_lang, target_lang, preprocess, max_size=500)\n",
    "print(train_data[:5])   # to see the first 5 rows of train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load or create the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dict_path = os.path.join(pretrained_model_dir, f'dict.{source_lang}.txt')\n",
    "target_dict_path = os.path.join(pretrained_model_dir, f'dict.{target_lang}.txt')\n",
    "\n",
    "source_dict = load_or_create_dictionary(\n",
    "    source_dict_path,\n",
    "    train_data['source_tokenized'],\n",
    "    reset=False,    # set reset to True if you're changing the data or the preprocessing\n",
    ")\n",
    "print(source_dict.words[:100])   # print the first 100 words in the source vocabulary\n",
    "\n",
    "target_dict = load_or_create_dictionary(\n",
    "    target_dict_path,\n",
    "    train_data['target_tokenized'],\n",
    "    reset=False,\n",
    ")\n",
    "print(target_dict.words[:100])\n",
    "\n",
    "print('source vocab size:', len(source_dict))\n",
    "print('target vocab size:', len(target_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use the dictionaries to map tokens to indices. The training set is also sorted by length for more efficient batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binarize(train_data, source_dict, target_dict, sort=True)\n",
    "binarize(valid_data, source_dict, target_dict, sort=False)\n",
    "binarize(test_data, source_dict, target_dict, sort=False)\n",
    "print(train_data[:5])  # print the first 5 rows of train_data\n",
    "# The 'source_bin' and 'target_bin' columns contain the sequences of indices\n",
    "# Indices of 2 correspond to the EOS token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_size={}, valid_size={}, test_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n",
    "    len(train_data),\n",
    "    len(valid_data),\n",
    "    len(test_data),\n",
    "    train_data['source_len'].min(),\n",
    "    train_data['source_len'].max(),\n",
    "    train_data['source_len'].mean(),\n",
    "))\n",
    "\n",
    "print('Train source length distribution:')\n",
    "# The 90th percentile indicates the point where 90% percent of the data have values lower than this number.\n",
    "# We see that 90% of training examples have 15 source words or less\n",
    "# and 99% of all training examples have 30 source words or less.\n",
    "print(train_data['source_len'].quantile([0.5, 0.9, 0.95, 0.99, 0.999]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_percentage(column):\n",
    "    total = sum(len(ids) for ids in column)\n",
    "    unk = sum((ids == data.UNK_IDX).sum() for ids in column)\n",
    "    return unk / total\n",
    "\n",
    "print(f\"OOV source words: {unk_percentage(train_data['source_bin']):.2%}\")\n",
    "print(f\"OOV target words: {unk_percentage(train_data['target_bin']):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build batches. The training batches are automatically shuffled before each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30       # maximum 30 tokens per sentence (longer sequences will be truncated)\n",
    "BATCH_SIZE = 512   # maximum 512 tokens per batch (decrease if you get out-of-memory errors,\n",
    "# increase to speed up training)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = BatchIterator(train_data, source_lang, target_lang, BATCH_SIZE, max_len=MAX_LEN, shuffle=True)\n",
    "valid_iterator = BatchIterator(valid_data, source_lang, target_lang, BATCH_SIZE, max_len=MAX_LEN, shuffle=False)\n",
    "test_iterator = BatchIterator(test_data, source_lang, target_lang, BATCH_SIZE, max_len=MAX_LEN, shuffle=False)\n",
    "\n",
    "print('Example of training batch:')\n",
    "print(next(iter(train_iterator)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sequence-to-sequence models\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence, seq2seq, or Encoder-Decoder model`, is a model consisting of usually two neural networks called the encoder and decoder (http://arxiv.org/abs/1409.3215, https://arxiv.org/abs/1406.1078v3). The encoder reads\n",
    "an input sequence and outputs a vector representation, and the decoder reads\n",
    "that vector representation to produce an output sequence. Essentially, all we need is some mechanism to read the source sentence and create an encoding and some mechanism to read the encoding and decode it to the target language. \n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"I am not the\n",
    "black cat\" → \"Je ne suis pas le chat noir\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a basic seq2seq model, the encoder creates a single vector which, in the\n",
    "ideal case, encodes the meaning of the input sequence into a single\n",
    "vector — a single point in some N-dimensional space of sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "The encoder is anything which takes in a sentence and gives us a vector representation of this sentence. \n",
    "\n",
    "The encoder of a seq2seq network can be an RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "However, we will start with a simpler Bag-of-Words encoder and then move on to more complex encoders.\n",
    "This encoder is a simple feed-forward network applied independently at each source position (i.e., on each word embedding). The outputs of the last encoder layer are then summed into a single vector and used as input by the RNN decoder.\n",
    "\n",
    "### Bag-of-Words encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_encoder = models.BOW_Encoder(\n",
    "    source_dict=source_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    reduce='sum',\n",
    ")\n",
    "\n",
    "print(bow_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The decoder\n",
    "\n",
    "The decoder is another network that takes the encoder's output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "### Decoder without attention\n",
    "\n",
    "In the simplest seq2seq decoder we use only the last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and the encoder's context vector and it updates its internal state, which is then used to predict the next word. The initial input token is the start-of-sequence <SOS> token. The next inputs are the decoder's own predictions (at test time) or the ground-truth tokens (at train time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_decoder = models.RNN_Decoder(\n",
    "    target_dict=target_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(bow_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = models.EncoderDecoder(\n",
    "    bow_encoder,\n",
    "    bow_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=not cpu,\n",
    "    max_len=MAX_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "`train_model` trains a model for a given number of epochs. It will evaluate this model on the validation sets after each training epoch, and save a checkpoint if the model has improved.\n",
    "\n",
    "`evaluate_model` computes validation loss and chrF.\n",
    "\n",
    "chrF (https://aclanthology.org/W16-2341/) is a string-based metric, less known than BLEU, but which has been shown to outperform BLEU (i.e., to correlate better with human judgment). It also has the advantage that, because it is at the character-level, it does not rely on word tokenization and is more language-independent than BLEU.\n",
    "\n",
    "However, (hopefully) researchers will gradually move away from string-based metrics, to use the superior learned metrics (e.g., BARTScore: https://arxiv.org/abs/2106.11520).\n",
    "\n",
    "`utils.plot_loss` plots the model's performance on the training and validation set (train loss and validation loss/chrF). It can be used to diagnose overfitting issues: if the training loss continues decreasing while the validation loss increases, this can mean that we are not doing enough regularization (e.g., `dropout`) or that the model is just too big for this tiny training corpus.\n",
    "\n",
    "On the other hand, if the training loss seems to stagnate, this can mean that we're doing too much regularization or not using the right learning rate schedule: the initial learning rate is either too large or too small, or a different scheduler should be used. By default, we're using ReduceLROnPlateau, which divides the learning rate by 10 when validation chrF hasn't improved (by at least a 0.5 margin) over the previous best. Depending on the model, this can be either too aggressive or not aggressive enough.\n",
    "\n",
    "`translate` lets you use the model to translate a single sentence (without having to preprocess it beforehand). Note that for simplicity, we do \"greedy\" decoding: we generate the highest-probability word at each step without seeking to maximize the sequence-level score. A slightly better and often used decoding algorithm is \"beam search\", which maintains a fixed number of hypotheses at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, *test_or_valid_iterators, record=False):\n",
    "    \"\"\"\n",
    "    Evaluate given models with given test or validation sets. This will compute both chrF and validation loss.\n",
    "    \n",
    "    model: instance of models.EncoderDecoder\n",
    "    test_or_valid_iterators: list of BatchIterator\n",
    "    record: save scores in the model checkpoint\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    model.half()  # half-precision decoding is faster on some GPUs (i.e., model parameters and activations\n",
    "    # are stored in float16 format instead of float32)\n",
    "    \n",
    "    # Compute chrF and valid loss over all test or validation sets\n",
    "    for iterator in test_or_valid_iterators:\n",
    "        loss = 0\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        \n",
    "        for batch in iterator:\n",
    "            loss += model.eval_step(batch) / len(iterator)\n",
    "            hyps, _ = model.translate(batch)\n",
    "            hypotheses += [postprocess(hyp) for hyp in hyps]  # detokenize\n",
    "            references += batch['reference']\n",
    "        \n",
    "        chrf = sacrebleu.corpus_chrf(hypotheses, [references]).score\n",
    "\n",
    "        src, tgt = iterator.source_lang, iterator.target_lang\n",
    "        print(f'{src}-{tgt}: loss={loss:.2f}, chrF={chrf:.2f}')\n",
    "        if record:  # store the metrics in the model checkpoint\n",
    "            model.record(f'{src}_{tgt}_loss', loss)\n",
    "            model.record(f'{src}_{tgt}_chrf', chrf)\n",
    "        \n",
    "        scores.append(chrf)\n",
    "\n",
    "    # Average the validation chrF scores\n",
    "    score = sum(scores) / len(scores)\n",
    "    return score\n",
    "\n",
    "\n",
    "def train_model(model, train_iterator, valid_iterators, checkpoint_path=None, epochs=10):\n",
    "    \"\"\"\n",
    "    Train given model for the given number of epochs.\n",
    "    The best performing checkpoint (according to average chrF on 'valid_iterators') will be saved\n",
    "    under 'checkpoint_path'.\n",
    "    \n",
    "    By default, the optimizer, epoch counter and learning rate scheduler are not reset.\n",
    "    This means that this function can be called several times:\n",
    "        train_model(epochs=2) is equivalent to train_model(epochs=1); train_model(epochs=1)\n",
    "    Call model.reset_optimizer() to reset the model to its initial optimization settings.\n",
    "    \n",
    "    model: instance of models.EncoderDecoder\n",
    "    train_iterator: instance of data.BatchIterator used for generating training batches\n",
    "    valid_iterators: list of BatchIterator used for evaluation\n",
    "    checkpoint_path: path where the model will be saved (None to not save any checkpoint)\n",
    "    epochs: iterate this many times over train_iterator\n",
    "    \"\"\"\n",
    "    epochs += (model.epoch - 1)\n",
    "\n",
    "    reset_seed()\n",
    "    \n",
    "    best_score = -1\n",
    "    while model.epoch <= epochs:\n",
    "        model.float()  # half-precision training is unstable, we do mixed-precision internally with torch.autocast instead\n",
    "        \n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print(f'Epoch [{model.epoch}/{epochs}]')\n",
    "\n",
    "        # Iterate over training batches for one epoch\n",
    "        with tqdm(enumerate(train_iterator), total=len(train_iterator)) as t:\n",
    "\n",
    "            for i, batch in t:\n",
    "                running_loss += model.train_step(batch)\n",
    "                model.scheduler_step(end_of_epoch=False)\n",
    "                t.postfix = f' loss={running_loss / (i + 1):.3f}'\n",
    "\n",
    "        # Mean training loss for this epoch\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "\n",
    "        print(f'train_loss={epoch_loss:.3f}, time={time.time() - start:.2f}')\n",
    "        model.record('train_loss', epoch_loss)\n",
    "\n",
    "        score = evaluate_model(model, *valid_iterators, record=True)\n",
    "\n",
    "        # Update the model's learning rate based on current performance.\n",
    "        # This scheduler divides the learning rate by 10 if chrF does not improve.\n",
    "        model.scheduler_step(score, end_of_epoch=True)\n",
    "\n",
    "        # Save a model checkpoint if it has the best validation chrF so far\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            if checkpoint_path is not None:\n",
    "                model.save(checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(f'Training completed. Best chrF is {best_score:.2f}')\n",
    "\n",
    "\n",
    "def make_batch(sources, dictionary, prefix=None, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Create a batch from given source sentences\n",
    "    `prefix` is an optional target-side prefix (e.g., target-side a language code for the NLLB models)\n",
    "    \"\"\"\n",
    "    batch = [\n",
    "        {\n",
    "            'source': dictionary.txt2vec(source, add_eos=True),\n",
    "            'prefix': dictionary.txt2vec(prefix),\n",
    "        }\n",
    "        for source in sources\n",
    "    ]\n",
    "    return data.collate(batch, max_len)\n",
    "\n",
    "\n",
    "def get_translations(model, sentences, preprocess=preprocess, source_lang=source_lang, target_lang=target_lang,\n",
    "                     max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Translate given sentences with given model\n",
    "    \"\"\"\n",
    "    sentences_tok = []\n",
    "    prefix_tok = None  # should be the same for all sentences\n",
    "    for sentence in sentences:\n",
    "        tokenized = preprocess(\n",
    "            sentence,\n",
    "            target_line=None,\n",
    "            source_lang=source_lang,\n",
    "            target_lang=target_lang,\n",
    "        )  # returns (tokenized source, tokenized target, optional target prefix)\n",
    "        sentences_tok.append(tokenized[0])\n",
    "        if len(tokenized) == 3:\n",
    "            prefix_tok = tokenized[-1]\n",
    "        \n",
    "    batch = make_batch(sentences_tok, model.source_dict, prefix=prefix_tok, max_len=max_len)\n",
    "    predictions, attention = model.translate(batch)\n",
    "    predictions_detok = [postprocess(prediction) for prediction in predictions]\n",
    "    if prefix_tok:\n",
    "        predictions = [f'{prefix_tok} {prediction}' for prediction in predictions]\n",
    "    return {\n",
    "        'source': sentences,\n",
    "        'source_tok': sentences_tok,\n",
    "        'predictions': predictions,\n",
    "        'predictions_detok': predictions_detok,\n",
    "        'attention': attention,\n",
    "    }\n",
    "\n",
    "\n",
    "def pivot_translation(model, sentences, preprocess, source_lang, target_lang, pivot_lang='en'):\n",
    "    \"\"\"\n",
    "    Translate given sentences from `source_lang` to `target_lang` by pivot translation through `pivot_lang`\n",
    "    \"\"\"\n",
    "    output = get_translations(model, sentences, preprocess, source_lang=source_lang, target_lang=pivot_lang)\n",
    "    output = output['predictions_detok']\n",
    "    output = get_translations(model, output, preprocess, source_lang=pivot_lang, target_lang=target_lang)\n",
    "    output = output['predictions_detok']\n",
    "    return output\n",
    "\n",
    "\n",
    "def translate(model, sentence, preprocess=preprocess, source_lang=source_lang, target_lang=target_lang,\n",
    "              google_translate=True, plot_attention=True, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Translate given sentence with given model and print the outputs.\n",
    "    Also show translation outputs by Google Translate for comparison.\n",
    "\n",
    "    sentence (str): sentence to translate\n",
    "    preprocess: function used to tokenize the input sentence\n",
    "    source_lang (str): source language code (used for Google Translate and as a parameter to \"preprocess\")\n",
    "    target_lang (str): target language code (used for Google Translate and as a parameter to \"preprocess\")\n",
    "    google_translate: show translations by Google Translate\n",
    "    plot_attention: show the encoder-decoder attention matrix as a heatmap\n",
    "    \"\"\"\n",
    "    output = get_translations(\n",
    "        model,\n",
    "        [sentence],\n",
    "        preprocess=preprocess,\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "    \n",
    "    print('Source:                ', sentence)\n",
    "    print('Tokenized source:      ', output['source_tok'][0])\n",
    "    print('Prediction:            ', output['predictions'][0])\n",
    "    print('Detokenized prediction:', output['predictions_detok'][0])\n",
    "    print()\n",
    "    \n",
    "    if google_translate:\n",
    "        print('Google Translate ({}->{}):               {}'.format(\n",
    "            source_lang,\n",
    "            target_lang,\n",
    "            google_translator.translate(output['source'][0], src=source_lang, dest=target_lang).text,\n",
    "        ))\n",
    "        print('Google Translate on prediction ({}->{}): {}'.format(\n",
    "            target_lang,\n",
    "            source_lang,\n",
    "            google_translator.translate(output['predictions_detok'][0], src=target_lang, dest=source_lang).text,\n",
    "        ))\n",
    "\n",
    "    if plot_attention and output['attention'] is not None:\n",
    "        utils.plot_attention(output['source_tok'][0], output['predictions'][0], output['attention'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with BOW encoder and RNN decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "# Note that you can load the pre-trained model, then re-run this cell with train_again=True to continue training it\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'bow.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'bow.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    bow_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(bow_model, train_iterator, [valid_iterator],\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)\n",
    "\n",
    "utils.plot_loss(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(bow_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate some English sentence with the model\n",
    "translate(bow_model, 'Do you like dogs?')\n",
    "# The Google Translate outputs are shown for reference to non-French speakers:\n",
    "# - The en->fr output is a high-quality translation of the input sentence\n",
    "# - The fr->en output is a translation back into English of our model's French translation (so that you can assess its quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest limitation of a Bag-of-Words encoder is that it is insensitive to word order: when shuffling the words in the previous sentence, you get the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(bow_model, 'Do dogs like you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate(bow_model, \"The mouse ate the cat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN encoder + RNN decoder\n",
    "\n",
    "Now let's look at a more powerful model, which also uses an RNN to encode the source sequence. Contrary to the Bag-of-Words encoder, it is sensitive to word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = models.RNN_Encoder(\n",
    "    source_dict=source_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(rnn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_decoder = models.RNN_Decoder(\n",
    "    target_dict=target_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(rnn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = models.EncoderDecoder(\n",
    "    rnn_encoder,\n",
    "    rnn_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=not cpu,\n",
    "    max_len=MAX_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with RNN encoder and RNN decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "# Note that you can load the pre-trained model, then re-run this cell with train_again=False to continue training it\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'rnn.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'rnn.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    rnn_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(rnn_model, train_iterator, [valid_iterator],\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)\n",
    "\n",
    "utils.plot_loss(rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(rnn_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, \"She's five years older than me.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrary to the BoW encoder, an RNN is sensitive to word ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, \"Do you like dogs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, \"Do dogs like you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(rnn_model, \"The mouse ate the cat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "However, researchers observed that this type of RNN encoder/decoder model is hard to train and not very good to deal with long sequences, because the entire input has to be encoder into a single fixed-size vector, no matter its length. Because of this, attention mechanisms were introduced between the encoder and the decoder (https://arxiv.org/abs/1409.0473): the decoder can look at different positions in the encoder depending on its own current state. This usually implemented as a weighted average over encoder states, whose weights are computed with a learnable feed-forward network taking an encoder state and a decoder state as input.\n",
    "\n",
    "But nowadays, the preferred architecture is the [Transformer](https://arxiv.org/abs/1706.03762) which uses a more complex \"multi-head attention\" mechanism, and not only between, but also within the encoder and the decoder (AKA \"self-attention\") as a replacement to the recursion of RNNs. Transformers are basically deep feed-forward networks where each layer has an attention mechanism over the preceding layer. Transformers are considerably faster to train than RNNs because all the states of a given layer can be computed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = models.TransformerEncoder(\n",
    "    source_dict=source_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    heads=4,\n",
    ")\n",
    "\n",
    "print(transformer_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder = models.TransformerDecoder(\n",
    "    target_dict=target_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=1,\n",
    "    heads=4,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(transformer_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = models.EncoderDecoder(\n",
    "    transformer_encoder,\n",
    "    transformer_decoder,\n",
    "    lr=0.0005,\n",
    "    use_cuda=not cpu,\n",
    "    max_len=MAX_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this notebook, we're using the same learning rate scheduler for all models:\n",
    "`torch.optim.lr_scheduler.ReduceLROnPlateau`, which reduces the learning rate when the validation score (chrF)\n",
    "does not increase enough.\n",
    "Feel free to experiment with other schedulers, using the `scheduler_fn` and `scheduler_args` parameters.\n",
    "\n",
    "\n",
    "For example:\n",
    "```\n",
    "transformer_model = models.EncoderDecoder(\n",
    "    transformer_encoder,\n",
    "    transformer_decoder,\n",
    "    lr=0.0005,\n",
    "    use_cuda=not cpu,\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ExponentialLR,\n",
    "    scheduler_args={'gamma': 0.5},\n",
    ")\n",
    "```\n",
    "\n",
    "Transformers are often trained with warmup: starting with a small learning rate, increasing it up to a maximum value for the first N steps, them slowly decreasing it. Such a scheduler is implemented as `models.WarmupLR`.\n",
    "\n",
    "Deeper models can also be trained (Transformer encoders and decoders are often at least 6 layers). Regularization (`dropout` parameter) might need to be modified accordingly to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Transformer model (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epochs\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "# Note that you can load the pre-trained model, then re-run this cell with train_again=False to continue training it\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'transformer.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(pretrained_model_dir, 'transformer.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    transformer_model.load(checkpoint_path)   # trained for 10 epochs\n",
    "else:\n",
    "    train_model(transformer_model, train_iterator, [valid_iterator],\n",
    "                epochs=2,\n",
    "                checkpoint_path=checkpoint_path)\n",
    "    \n",
    "utils.plot_loss(rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chrF on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(transformer_model, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model\n",
    "The `translate` function also plots the encoder-decoder attention matrix. Note that this does an average over all attention heads and only at the last decoder layer. The vertical axis shows the encoder positions (and corresponding source words) and the horizontal axis shows the decoder positions (and the words that were generated at these positions). And each cell gives an average attention weight (value between 0 and 1) between these positions.\n",
    "\n",
    "Interestingly, even though the model is not trained with any prior regarding this alignment, encoder-decoder attention matrices often display linguistically-plausible alignments between the source and the target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"Look, there's a cat in the kitchen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"She's five years older than me.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, 'I know that the last thing you want to do is help me.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Controlling generation with input tags\n",
    "\n",
    "Some aspects of generation can be controlled thanks to special tokens in the input. For instance multi-domain models can be trained and used with source-side domain tags (https://aclanthology.org/R17-1049).\n",
    "\n",
    "## Politeness control\n",
    "\n",
    "This work https://aclanthology.org/N16-1005/ used special tokens to control the politeness of the output.\n",
    "\n",
    "We will implement this approach for English-French translation, to control the use of *\"tu\"* vs *\"vous\"* pronouns, which are formal/informal translations of \"you\".\n",
    "\n",
    "We only need to partition the training data into formal vs informal splits, by looking for occurrences of *\"tu\"* and *\"vous\"*. Then, add source-side control tags depending on the politeness level of the target, and train the model with this.\n",
    "At test time, we only need to put the right control tag and the model will know how to interpret it to pick the right level of politeness.\n",
    "\n",
    "\n",
    "### Politeness detector\n",
    "\n",
    "As we only rely on the \"politeness control token,\" it is necessary to prepare distinctive polite and non-polite training samples from the corpus.\n",
    "\n",
    "While a lot of different aspects of French grammar can be considered here, to start with, we pick sentences that contain *\"tu\"* and *\"vous\"* — both meaning \"you\"  in English — and label them as \"non-polite\" and \"polite,\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_punct(line):\n",
    "    \"\"\"\n",
    "    Splits according to punctuation symbols: \"Hello, world!\" -> [\"Hello\", \",\", \" \", \"world\", \"!\", \"\"]\n",
    "    This can be reverted by: ''.join(split_punct(line))\n",
    "    \"\"\"\n",
    "    return re.split(r'(\\W)', line or '')\n",
    "\n",
    "def is_formal(line):\n",
    "    \"\"\"\n",
    "    Contains formal French translations of \"you\"\n",
    "    \"\"\"\n",
    "    tokens = split_by_punct(line)\n",
    "    # Modify this regex to match other formal pronouns (e.g., votre/vos)\n",
    "    return any(re.fullmatch(r'vous', token, re.IGNORECASE) for token in tokens)\n",
    "\n",
    "def is_informal(line):\n",
    "    \"\"\"\n",
    "    Contains informal French translations of \"you\"\n",
    "    \"\"\"\n",
    "    tokens = split_by_punct(line)\n",
    "    # Modify this regex to match other informal pronouns (e.g., ton/ta/tes)\n",
    "    return any(re.fullmatch(r'tu', token, re.IGNORECASE) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding politeness control tags\n",
    "\n",
    "When we identify sentences that are either polite or non-polite, we can attach corresponding control tags in front of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_formal(source_line, target_line=None, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Tokenizes the given line pair and prepends the <formal> source-side tag \n",
    "    \"\"\"\n",
    "    source_line, target_line = preprocess(source_line, target_line)\n",
    "    source_line = f'<formal> {source_line}'\n",
    "    return source_line, target_line\n",
    "\n",
    "def preprocess_informal(source_line, target_line=None, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Tokenizes the given line pair and prepends the <informal> source-side tag \n",
    "    \"\"\"\n",
    "    source_line, target_line = preprocess(source_line, target_line)\n",
    "    source_line = f'<informal> {source_line}'\n",
    "    return source_line, target_line\n",
    "\n",
    "def preprocess_formal_or_informal(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for politeness control:\n",
    "    - keep only line pairs whose target side has French formal or informal pronouns\n",
    "    - prepend politeness control tags to the source side\n",
    "    \"\"\"\n",
    "    if is_formal(target_line):\n",
    "        return preprocess_formal(source_line, target_line)\n",
    "    elif is_informal(target_line):\n",
    "        return preprocess_informal(source_line, target_line)\n",
    "    else:  # this line pair in neither formal nor informal\n",
    "        # This example will be filtered out by load_dataset (uncomment below to keep it, without a control tag):\n",
    "        # return preprocess(source_line, target_line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and loading the dataset\n",
    "\n",
    "Finally, we can filter and load the dataset by passing the `preprocess_formal_or_informal` function to `load_dataset`.\n",
    "This will keep only the line pairs that contain formal or informal pronouns and preprocess the sources to add control tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same dataset as before\n",
    "train_path = os.path.join(data_dir, 'train.en-fr')\n",
    "valid_path = os.path.join(data_dir, 'valid.en-fr')\n",
    "\n",
    "# But preprocess it to keep only line pairs that use tu/vous pronouns and to append control tags\n",
    "train_data_politeness = load_dataset(\n",
    "    train_path, 'en', 'fr',\n",
    "    preprocess=preprocess_formal_or_informal,\n",
    ")\n",
    "\n",
    "valid_data_politeness = load_dataset(\n",
    "    valid_path, 'en', 'fr',\n",
    "    preprocess=preprocess_formal_or_informal,\n",
    "    max_size=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with politeness tags \n",
    "\n",
    "As we are introducing new words in the vocabulary (i.e., the control tokens), we need to add them to our pretrained model's existing vocabulary.\n",
    "\n",
    "Here, we replace the last two most infrequent tokens so that we do not need to resize the vocabulary and embeddings.\n",
    "\n",
    "Note that the replaced words will now be mapped to `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dict = transformer_model.source_dict\n",
    "\n",
    "# Replace some infrequent tokens with the new control tokens (these words will now be mapped to <unk>)\n",
    "# This is a bit dirty, but this way we don't have to resize the pretrained model's vocabulary and embeddings\n",
    "source_dict[len(source_dict) - 2] = '<formal>'\n",
    "source_dict[len(source_dict) - 1] = '<informal>'\n",
    "\n",
    "# Binarize the training and validation data with these vocabularies\n",
    "binarize(train_data_politeness, source_dict, target_dict, sort=True)\n",
    "binarize(valid_data_politeness, source_dict, target_dict, sort=False)\n",
    "\n",
    "# You can see that the training source examples now start with special tokens.\n",
    "print(train_data_politeness[:5])\n",
    "\n",
    "print('train_size={}, valid_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n",
    "    len(train_data_politeness),\n",
    "    len(valid_data_politeness),\n",
    "    train_data_politeness['source_len'].min(),\n",
    "    train_data_politeness['source_len'].max(),\n",
    "    train_data_politeness['source_len'].mean(),\n",
    "))\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator_politeness = BatchIterator(\n",
    "    train_data_politeness, 'en', 'fr',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_iterator_politeness = BatchIterator(\n",
    "    valid_data_politeness, 'en', 'fr',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the EN-FR pretrained Transformer model with the new data\n",
    "new_checkpoint_path = os.path.join(model_root, 'en-fr', 'polite-transformer.pt')\n",
    "transformer_model.reset_optimizer()\n",
    "# Uncomment below to reload the pre-trained model\n",
    "# transformer_model.load(os.path.join(pretrained_model_dir, 'transformer.pt'), reset_optimizer=True)\n",
    "train_model(\n",
    "    transformer_model,\n",
    "    train_iterator_politeness,\n",
    "    [valid_iterator_politeness],\n",
    "    new_checkpoint_path,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your polite Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"Would you lend me your bicycle?\", preprocess_formal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(transformer_model, \"Would you lend me your bicycle?\", preprocess_informal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "Can you improve the `is_formal` and `is_informal` functions to find more training examples?\n",
    "For instance, French possessives (*ton/ta/tes*, *votre/vos*) also have this formality distinction.\n",
    "\n",
    "By default, `preprocess_formal_or_informal` will exclude any training example that is neither formal nor informal. This results in a very small and biased dataset. The resulting model will also catastrophically forget how to translate sentences that do not start with politeness tags. It may be beneficial (to avoid overfitting and catastrophic forgetting) to also include regular training examples, without any politeness tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling gender\n",
    "\n",
    "One known issue of machine translation models (and other NLP models) is that they tend to exhibit gender biases, caused by the same biases appearing in the training data. For instance, in case of ambiguity, a doctor is more likely to be translated as masculine and a nurse as feminine.\n",
    "\n",
    "For instance *\"Dr. Dupont is very skilled\"* -> *\"Le Dr. Dupont est très compétent\"* (*\"compétent\"* is masculine, the feminine form is *\"compétente\"*).\n",
    "\n",
    "You will now use control tags to control the gender of the translation. Sentences starting with `<feminine>` will be translated with the feminine pronoun *\"elle\"* and translations of sentences starting with `<masculine>` will use the masculine pronoun *\"il\"*.\n",
    "\n",
    "Unfortunately, we don't have a mainstream gender-neutral pronoun in French (like *\"they\"* in English).\n",
    "An option called \"inclusive writing\" consists in writing both pronouns (e.g., *\"il/elle\"*), but there aren't\n",
    "many natural occurrences of this in existing NLP datasets yet, so for simplicity we will stick to binary masculine/feminine.\n",
    "\n",
    "You can mostly mirror the \"Politeness control\" task and change the regular expressions. Don't forget the `re.IGNORECASE` flag to also match capitalized words (e.g., both *\"Elle\"* and *\"elle\"*).\n",
    "\n",
    "A notable difference with the previous task is that we now want to impose some feature in the output that may be different to what appears in the input. For instance, `<feminine> He eats apples` should translate as `Elle mange des pommes` Because such things rarely occur naturally in MT data (contrary to politeness ambiguities), we will need to do some data augmentation. This can be achieved by randomly swapping the masculine or feminine pronouns in the English source lines. Modify the `feminize` and `masculinize` functions to do this.\n",
    "\n",
    "Set `implemented = True` once you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implemented = False  # set to True once you've implemented all the functions below\n",
    "\n",
    "def is_feminine(line):\n",
    "    \"\"\"\n",
    "    Contains the French feminine pronoun \"elle\"\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def is_masculine(line):\n",
    "    \"\"\"\n",
    "    Contains the French masculine pronoun \"il\"\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def preprocess_feminine(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for feminine line pairs: the source side will have a special <feminine> token\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def preprocess_masculine(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for masculine line pairs: the source side will have a special <masculine> token\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def feminize(line):\n",
    "    \"\"\"\n",
    "    Change the English pronouns in `line` to be feminine\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def masculinize(line):\n",
    "    \"\"\"\n",
    "    Change the English pronouns in `line` to be masculine\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def preprocess_masculine_or_feminine(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Preprocessing function for gender control:\n",
    "    - add the <feminine> source tag to sentences pairs whose target side is feminine\n",
    "    - add the <masculine> source tag to sentences pairs whose target side is masculine\n",
    "    - do data augmentation to swap the source-side gender with probability 0.5\n",
    "    \"\"\"\n",
    "    if is_feminine(target_line):\n",
    "        if np.random.rand() < 0.5:\n",
    "            source_line = masculinize(source_line)\n",
    "        return preprocess_feminine(source_line, target_line)\n",
    "    elif is_masculine(target_line):\n",
    "        if np.random.rand() < 0.5:\n",
    "            source_line = feminize(source_line)\n",
    "        return preprocess_masculine(source_line, target_line)\n",
    "    else:\n",
    "        # return preprocess(source_line, target_line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the previous functions have been filled in, the following can be run without modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if implemented:\n",
    "    reset_seed()\n",
    "\n",
    "    train_data_gender = load_dataset(train_path, 'en', 'fr', preprocess=preprocess_masculine_or_feminine)\n",
    "    valid_data_gender = load_dataset(valid_path, 'en', 'fr', preprocess=preprocess_masculine_or_feminine, max_size=500)\n",
    "\n",
    "    source_dict = transformer_model.source_dict\n",
    "\n",
    "    # Replace some infrequent tokens with the new control tokens (these words will now be mapped to <unk>)\n",
    "    # This is a bit dirty, but this way we don't have to resize the pretrained model's vocabulary and embeddings\n",
    "    source_dict[len(source_dict) - 2] = '<feminine>'\n",
    "    source_dict[len(source_dict) - 1] = '<masculine>'\n",
    "\n",
    "    # Binarize the training and validation data with these vocabularies\n",
    "    binarize(train_data_gender, source_dict, target_dict, sort=True)\n",
    "    binarize(valid_data_gender, source_dict, target_dict, sort=False)\n",
    "\n",
    "    # You can see that the training source examples now start with special tokens.\n",
    "    print(train_data_gender[:5])\n",
    "\n",
    "    print('train_size={}, valid_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n",
    "        len(train_data_gender),\n",
    "        len(valid_data_gender),\n",
    "        train_data_gender['source_len'].min(),\n",
    "        train_data_gender['source_len'].max(),\n",
    "        train_data_gender['source_len'].mean(),\n",
    "    ))\n",
    "\n",
    "    train_iterator_gender = BatchIterator(\n",
    "        train_data_gender, 'en', 'fr',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_len=MAX_LEN,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_iterator_gender = BatchIterator(\n",
    "        valid_data_gender, 'en', 'fr',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_len=MAX_LEN,\n",
    "        shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if implemented:\n",
    "    # Finetune the EN-FR pretrained Transformer model with the new data\n",
    "    new_checkpoint_path = os.path.join(model_root, 'en-fr', 'gender-controllable-transformer.pt')\n",
    "    transformer_model.reset_optimizer()\n",
    "    # Uncomment below to reload the pre-trained model:\n",
    "    # transformer_model.load(os.path.join(pretrained_model_dir, 'transformer.pt'), reset_optimizer=True)\n",
    "    train_model(transformer_model, train_iterator_gender, [valid_iterator_gender], new_checkpoint_path, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if implemented:\n",
    "    translate(transformer_model, \"She is attending a Winter school.\", preprocess_masculine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if implemented:\n",
    "    translate(transformer_model, \"She is attending a Winter school.\", preprocess_feminine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multilingual Translation\n",
    "\n",
    "We will now look at multilingual translation, another trendy topic in machine translation. A single model can be trained to translate from multiple languages into multiple languages (https://aclanthology.org/Q17-1024/, https://arxiv.org/abs/2010.11125).\n",
    "This is done by having a single multilingual BPE tokenizer and dictionary, shared between all languages. The embedding matrix (and other model parameters) are also shared across languages. And this multilingual model is trained on multiple parallel datasets (e.g., EN->FR, FR->EN, DE->EN, EN->DE). Controlling the target language can be achieved by using special tokens, like for politeness control.\n",
    "\n",
    "Load a pre-trained **DE, FR <-> EN** model. The multilingual dictionary includes tokens for all three languages plus the language codes (`<lang:de>`, `<lang:en>`, `<lang:fr>`), which are prepended to each source sequence to identify the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model_dir = os.path.join(root_dir, 'pretrained_models', 'de-en-fr')\n",
    "\n",
    "multi_dict = data.Dictionary.load(os.path.join(multi_model_dir, 'dict.txt'))\n",
    "\n",
    "encoder = models.TransformerEncoder(source_dict=multi_dict, embed_dim=512, num_layers=2, heads=4)\n",
    "decoder = models.TransformerDecoder(\n",
    "    target_dict=multi_dict,\n",
    "    embed_dim=512, num_layers=1, heads=4,\n",
    "    embed_tokens=encoder.embed_tokens)  # tied embeddings (multilingual models usually have shared source/target embeddings)\n",
    "\n",
    "multi_model = models.EncoderDecoder(encoder, decoder, lr=0.0005, use_cuda=not cpu, max_len=MAX_LEN)\n",
    "\n",
    "checkpoint_path = os.path.join(multi_model_dir, 'transformer.pt')\n",
    "multi_model.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual evaluation\n",
    "\n",
    "Modify the `preprocess` function to automatically prepend language codes to all source sequences (when calling `translate`, or `load_data`).\n",
    "\n",
    "And load test sets in all language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_multi(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    source_line, target_line = preprocess(source_line, target_line)\n",
    "    source_line = f'<lang:{target_lang}> {source_line}'.strip()\n",
    "    return source_line, target_line\n",
    "\n",
    "test_sets = {}\n",
    "\n",
    "for pair in 'en-fr', 'fr-en', 'en-de', 'de-en', 'de-fr', 'fr-de':\n",
    "    src, tgt = pair.split('-')\n",
    "    path = os.path.join(data_dir, f'test.{pair}')\n",
    "    dataset = load_dataset(path, src, tgt, preprocess_multi, max_size=500)\n",
    "    binarize(dataset, source_dict=multi_dict, target_dict=multi_dict, sort=False)\n",
    "    iterator = BatchIterator(dataset, src, tgt, batch_size=BATCH_SIZE, max_len=MAX_LEN, shuffle=False)\n",
    "    test_sets[pair] = iterator\n",
    "    \n",
    "en_centric_test_sets = list(test_sets.values())[:4]\n",
    "non_en_centric_test_sets = list(test_sets.values())[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(multi_model, *en_centric_test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate accepts preprocess, source_lang and target_lang arguments\n",
    "translate(multi_model, \"She's five years older than me.\", preprocess_multi, source_lang='en', target_lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(multi_model, 'Sie ist fünf Jahre älter als ich.', preprocess_multi, source_lang='de', target_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot translation\n",
    "\n",
    "In theory, the model can do **zero-shot** translation, i.e., translate between German and French even though it has never seen German-French sentence pairs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(multi_model, *non_en_centric_test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, in practice zero-shot performance is very bad. Interact with the model to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(multi_model, 'Sie ist fünf Jahre älter als ich.', preprocess_multi, source_lang='de', target_lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(multi_model, 'Elle a cinq ans de plus que moi.', preprocess_multi, source_lang='fr', target_lang='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution to use such an English-centric model to translate between two languages that are not English is to do pivot translation. For instance, to translate from German to French, we can use the model to translate from German to English and then from English to French.\n",
    "However, this approach is twice as slow and it can propagate errors and some useful information may be lost in the first translation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "srcs = list(test_sets['de-fr'].data['source_data'])\n",
    "refs = list(test_sets['de-fr'].data['target_data'])\n",
    "hyps = pivot_translation(multi_model, srcs, preprocess_multi, 'de', 'fr', pivot_lang='en')\n",
    "import sacrebleu\n",
    "chrf = sacrebleu.corpus_chrf(hyps, [refs]).score\n",
    "print(f'de-fr (pivot): chrF={chrf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation to a new language pair\n",
    "\n",
    "Large-scale multilingual MT models are great as they can provide translations for multiple language pairs with just a single model.\n",
    "\n",
    "However, these models tend to be very large in terms of model parameters and require heavy computational power to train.\n",
    "\n",
    "Therefore, when adding a new language pair, instead of re-training the model from scratch using the previous and newly added corpora, it would be more efficient to finetune the pretrained model with the new data only.\n",
    "\n",
    "### Naive finetuning of the model\n",
    "In the above \"Multilingual Translation\" section, we observed poor zero-shot MT performance for the **DE, FR <-> EN** model.\n",
    "\n",
    "We saw that, while it is possible to do **DE <-> FR** translation, as the model has never seen such bilingual data, the performance was rather poor.\n",
    "\n",
    "Suppose now we want to explicitly train the model to additionally support the **DE -> FR** translation.\n",
    "One way is to load the corresponding dataset and finetune the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DE-FR training data\n",
    "src, tgt = 'de', 'fr'\n",
    "\n",
    "train_path = os.path.join(data_dir, f'train.{src}-{tgt}')\n",
    "valid_path = os.path.join(data_dir, f'valid.{src}-{tgt}')\n",
    "\n",
    "train_data = load_dataset(train_path, src, tgt, preprocess_multi, max_size=None)  # set max_size to 10000 for fast debugging\n",
    "valid_data = load_dataset(valid_path, src, tgt, preprocess_multi, max_size=500)\n",
    "\n",
    "binarize(train_data, source_dict=multi_dict, target_dict=multi_dict, sort=True)\n",
    "binarize(valid_data, source_dict=multi_dict, target_dict=multi_dict, sort=False)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = BatchIterator(train_data, src, tgt, batch_size=BATCH_SIZE, max_len=MAX_LEN, shuffle=True)\n",
    "valid_iterator = BatchIterator(valid_data, src, tgt, batch_size=BATCH_SIZE, max_len=MAX_LEN, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the entire model on DE-FR\n",
    "new_checkpoint_path = os.path.join(model_root, 'de-en-fr', 'finetuned-transformer.pt')\n",
    "train_model(multi_model, train_iterator, [valid_iterator], new_checkpoint_path, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic forgetting\n",
    "After the finetuning, we evaluate the model on FR-EN and DE-FR test sets. Unfortunately, this finetuning resulted in a drop in performance for FR-EN translation. This phenomenon of the model forgetting previously learned information upon learning new information is called \"catastrophic forgetting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate on FR-EN and DE-FR test sets. We see a drop in FR-EN performance (catastrophic forgetting)\n",
    "chrf = evaluate_model(multi_model, test_sets['fr-en'], test_sets['de-fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapter modules\n",
    "An alternative to finetuning and an effective way of avoiding the problem of catastrophic forgetting is the usage of adapter modules (https://arxiv.org/abs/1902.00751).\n",
    "\n",
    "An adapter module is generally a small feedforward network with a skip connection, inserted in each Transformer layer.\n",
    "\n",
    "The insertion of adapter modules incurs additional model parameters, but they are often kept small compared to the size of the original network.\n",
    "\n",
    "During adapter tuning, only the adapter modules are trained with the downstream task's data — in our case, the DE-FR data — while the rest of the model parameters are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AdapterTransformerDecoder, AdapterTransformerEncoderLayer\n",
    "\n",
    "class AdapterLayer(nn.Module):\n",
    "    # This class definition is just for show. Adapter layers are actually defined in models.py\n",
    "    # Same adapter architecture as in this paper: https://arxiv.org/abs/1909.08478\n",
    "    def __init__(self, input_dim, projection_dim):\n",
    "        \"\"\"\n",
    "        input_dim: Transformer model's embedding dimension\n",
    "        projection_dim: bottleneck dimension of the adapter (usually smaller than input_dim), can be tuned\n",
    "        to control the amount of new parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(input_dim, projection_dim)\n",
    "        self.up = nn.Linear(projection_dim, input_dim)\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        # initialize the adapter weights to small values, so that it computes the identity function\n",
    "        # (or close enough) at the beginning of training (i.e., it keeps the Transformer layer outputs mostly\n",
    "        # unchanged)\n",
    "        nn.init.uniform_(self.down.weight, -1e-6, 1e-6)\n",
    "        nn.init.uniform_(self.up.weight, -1e-6, 1e-6)\n",
    "        nn.init.zeros_(self.down.bias)\n",
    "        nn.init.zeros_(self.up.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer_norm(x)\n",
    "        # down projection to a bottleneck dimension\n",
    "        y = self.down(y)\n",
    "        # non-linearity\n",
    "        y = F.relu(y)\n",
    "        # up projection to the model's dimension\n",
    "        y = self.up(y)\n",
    "        # residual connection\n",
    "        return x + y\n",
    "\n",
    "class AdapterTransformerEncoder(models.TransformerEncoder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a Transformer Encoder with adapter modules (that will be plugged in after each Transformer layer)\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False  # only the adapters are trained\n",
    "\n",
    "    def add_adapter(self, id, projection_dim, select=False, overwrite=False):\n",
    "        # Create a new set of adapter modules\n",
    "        for layer in self.layers:\n",
    "            layer.add_adapter(id, projection_dim, overwrite=overwrite)\n",
    "        if select:\n",
    "            self.select_adapter(id)\n",
    "            \n",
    "    def select_adapter(self, id):\n",
    "        # Use this method to activate a specific set of adapters (e.g., 'de-fr')\n",
    "        # Set id=None to deactivate adapters (and use the initial Transformer model)\n",
    "        for layer in self.layers:\n",
    "            assert id is None or id in layer.adapters\n",
    "            layer.adapter_id = id\n",
    "\n",
    "    def build_layer(self, layer_id):\n",
    "        # This method can be modified to add adapters only at some layers (e.g., first encoder layer)\n",
    "        return AdapterTransformerEncoderLayer(self.embed_dim, self.heads, self.dropout_rate, self.ffn_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AdapterTransformerEncoder(\n",
    "    source_dict=multi_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    ")\n",
    "decoder = AdapterTransformerDecoder(\n",
    "    target_dict=multi_dict,\n",
    "    embed_dim=512,\n",
    "    num_layers=1,\n",
    "    heads=4,\n",
    "    embed_tokens=encoder.embed_tokens,\n",
    ")\n",
    "\n",
    "adapter_model = models.EncoderDecoder(encoder, decoder, lr=0.0005, use_cuda=not cpu, max_len=MAX_LEN)\n",
    "\n",
    "pretrained_checkpoint_path = os.path.join(multi_model_dir, 'transformer.pt')\n",
    "# Load the pre-trained model's parameters.\n",
    "# We reset the optimizer because its parameters do not match anymore and the learning rate might be too small.\n",
    "adapter_model.load(pretrained_checkpoint_path, reset_optimizer=True)\n",
    "\n",
    "encoder.add_adapter(\n",
    "    'de-fr',\n",
    "    projection_dim=64,  # bottleneck dimension of the adapters\n",
    ")\n",
    "decoder.add_adapter(\n",
    "    'de-fr',\n",
    "    projection_dim=64,\n",
    ")  # adapters can also be used only in the encoder or decoder\n",
    "\n",
    "new_checkpoint_path = os.path.join(model_root, 'de-en-fr', 'adapter-transformer.pt')\n",
    "\n",
    "# Show the number of trained parameters.\n",
    "# All Transformer parameters are frozen except the adapter parameters.\n",
    "total_params = 0\n",
    "trained_params = 0\n",
    "for name, param in adapter_model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trained_params += param.numel()\n",
    "print(f'Total parameters: {total_params}, trained parameters: {trained_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the DE-FR adapters and train them on the DE-FR data (the other parameters are frozen)\n",
    "# Note that you can do encoder.select_adapter(None) to train only decoder adapters\n",
    "encoder.select_adapter('de-fr')\n",
    "decoder.select_adapter('de-fr')\n",
    "train_model(adapter_model, train_iterator, [valid_iterator], new_checkpoint_path, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning on adapters for evaluation\n",
    "\n",
    "After adapter training, we can turn on the DE-FR adapters to do inference. The advantage over full finetuning, is that we can easily turn them off to translate in the other language pairs, and avoid the catastrophic forgetting issue.\n",
    "\n",
    "We can see that with just 200K new parameters (2% of the initial model's size) we can adapt to the DE-FR direction without hurting performance for the other language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the DE-FR adapters to translate in the DE-FR direction\n",
    "encoder.select_adapter('de-fr')\n",
    "decoder.select_adapter('de-fr')\n",
    "chrf = evaluate_model(adapter_model, test_sets['fr-en'], test_sets['de-fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate the adapters to use the initial model (e.g., to translate in the English-centric directions).\n",
    "encoder.select_adapter(None)\n",
    "decoder.select_adapter(None)\n",
    "chrf = evaluate_model(adapter_model, test_sets['fr-en'], test_sets['de-fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To automatically deactivate the adapters after using them. This also creates them if they don't exist\n",
    "with adapter_model.adapter('de-fr'):\n",
    "    chrf = evaluate_model(adapter_model, test_sets['de-fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "1. Can you train adapters to support more language pairs? (e.g., FR-DE). You can download data and train BPE models for more languages by modifying and re-running `scripts/download-data.sh` (warning: avoid re-running it for `de` and `fr` as it will generate different test splits)\n",
    "2. Can you achieve politeness control or gender control with adapters instead of control tags?\n",
    "3. Another technique to add new language pairs is to re-train (or finetune) the entire model on the new language pair's data **plus** the original language pairs. Train your own {DE,FR,EN} -> {DE,FR,EN} multilingual model. Tip: you can use `data.concatenate_datasets(dataset_list)` to concatenate multiple datasets (created by `load_dataset`) into a single one, or `data.MultilingualBatchIterator(iterator_list)` to merge several batch iterators (created by `BatchIterator`) into a single one. The first and second solutions will respectively result in heterogeneous and homogeneous batches (i.e., containing sentences pairs of multiple or a single language pair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. NLLB-200: a massively multilingual MT model\n",
    "\n",
    "Meta AI released several models under the name \"NLLB-200\", that support 202 languages, many of which are not covered by any commercial MT engine to data (https://arxiv.org/abs/2207.04672).\n",
    "The largest model is a mixture-of-experts model with 54B parameters, which is much too large for this notebook. But they also released smaller dense models of size: [3.3B, 1.3B, and 600M](https://github.com/facebookresearch/fairseq/tree/nllb#multilingual-translation-models). We will experiment here with the smallest model of 600M parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLLB takes quite a lot of GPU memory, move the previous models to CPU if you encounter OOM errors:\n",
    "# utils.free_gpu_memory()\n",
    "\n",
    "if not colab:\n",
    "    !bash scripts/download-nllb.sh\n",
    "\n",
    "nllb_model_dir = os.path.join(root_dir, 'pretrained_models', 'nllb')\n",
    "\n",
    "nllb_dict = data.Dictionary.load(os.path.join(nllb_model_dir, 'dict.txt'))\n",
    "\n",
    "NLLB_MAX_LEN = 100\n",
    "NLLB_BATCH_SIZE = 512\n",
    "\n",
    "# We initialize the model as a Transformer with adapters (even though it doesn't contain adapters yet),\n",
    "# as this will be useful to finetune it\n",
    "nllb_encoder = models.AdapterTransformerEncoder(\n",
    "    source_dict=nllb_dict,\n",
    "    embed_dim=1024,\n",
    "    ffn_dim=4096,\n",
    "    num_layers=12,\n",
    "    heads=16,\n",
    "    dropout=0.1,\n",
    "    checkpointing=False,  # set to True if you get OOM errors\n",
    ")\n",
    "nllb_decoder = models.AdapterTransformerDecoder(\n",
    "    target_dict=nllb_dict,\n",
    "    embed_dim=1024,\n",
    "    ffn_dim=4096,\n",
    "    num_layers=12,\n",
    "    heads=16,\n",
    "    embed_tokens=nllb_encoder.embed_tokens,  # tied embeddings (multilingual models usually have shared source/target embeddings)\n",
    "    dropout=0.1,\n",
    "    checkpointing=False,  # set to True if you get OOM errors\n",
    ")\n",
    "\n",
    "nllb_model = models.EncoderDecoder(\n",
    "    nllb_encoder,\n",
    "    nllb_decoder,\n",
    "    lr=0.0005,\n",
    "    max_len=NLLB_MAX_LEN,\n",
    "    use_cuda=not cpu,\n",
    "    scheduler=models.WarmupLR,\n",
    "    scheduler_args={'warmup': 500},\n",
    ")\n",
    "\n",
    "nllb_model.load(os.path.join(nllb_model_dir, '600M_distilled.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping 2-letter language codes to NLLB's language codes (e.g., fr -> fra_Latn, en -> eng_Latn)\n",
    "lang_code_mapping = {'af': 'afr_Latn', 'am': 'amh_Ethi', 'ar': 'arb_Arab', 'ast': 'ast_Latn', 'az': 'azj_Latn', 'ba': 'bak_Cyrl', 'be': 'bel_Cyrl', 'bn': 'ben_Beng', 'bs': 'bos_Latn', 'bg': 'bul_Cyrl', 'ca': 'cat_Latn', 'ceb': 'ceb_Latn', 'cs': 'ces_Latn', 'cy': 'cym_Latn', 'da': 'dan_Latn', 'de': 'deu_Latn', 'el': 'ell_Grek', 'en': 'eng_Latn', 'et': 'est_Latn', 'fi': 'fin_Latn', 'fr': 'fra_Latn', 'ff': 'fuv_Latn', 'gd': 'gla_Latn', 'ga': 'gle_Latn', 'gl': 'glg_Latn', 'gu': 'guj_Gujr', 'ht': 'hat_Latn', 'ha': 'hau_Latn', 'he': 'heb_Hebr', 'hi': 'hin_Deva', 'hr': 'hrv_Latn', 'hu': 'hun_Latn', 'hy': 'hye_Armn', 'ig': 'ibo_Latn', 'ilo': 'ilo_Latn', 'id': 'ind_Latn', 'is': 'isl_Latn', 'it': 'ita_Latn', 'jv': 'jav_Latn', 'ja': 'jpn_Jpan', 'kn': 'kan_Knda', 'ka': 'kat_Geor', 'kk': 'kaz_Cyrl', 'km': 'khm_Khmr', 'ko': 'kor_Hang', 'lo': 'lao_Laoo', 'ln': 'lin_Latn', 'lt': 'lit_Latn', 'lb': 'ltz_Latn', 'lg': 'lug_Latn', 'lv': 'lvs_Latn', 'ml': 'mal_Mlym', 'mr': 'mar_Deva', 'mk': 'mkd_Cyrl', 'mg': 'plt_Latn', 'mn': 'khk_Cyrl', 'my': 'mya_Mymr', 'nl': 'nld_Latn', 'no': 'nob_Latn', 'ne': 'npi_Deva', 'ns': 'nso_Latn', 'oc': 'oci_Latn', 'or': 'ory_Orya', 'pa': 'pan_Guru', 'fa': 'pes_Arab', 'pl': 'pol_Latn', 'pt': 'por_Latn', 'ps': 'pbt_Arab', 'ro': 'ron_Latn', 'ru': 'rus_Cyrl', 'si': 'sin_Sinh', 'sk': 'slk_Latn', 'sl': 'slv_Latn', 'sd': 'snd_Arab', 'so': 'som_Latn', 'es': 'spa_Latn', 'sq': 'als_Latn', 'sr': 'srp_Cyrl', 'ss': 'ssw_Latn', 'su': 'sun_Latn', 'sv': 'swe_Latn', 'sw': 'swh_Latn', 'ta': 'tam_Taml', 'tl': 'tgl_Latn', 'th': 'tha_Thai', 'tn': 'tsn_Latn', 'tr': 'tur_Latn', 'uk': 'ukr_Cyrl', 'ur': 'urd_Arab', 'uz': 'uzn_Latn', 'vi': 'vie_Latn', 'wo': 'wol_Latn', 'xh': 'xho_Latn', 'yi': 'ydd_Hebr', 'yo': 'yor_Latn', 'zh': 'zho_Hans', 'ms': 'zsm_Latn', 'zu': 'zul_Latn'}\n",
    "\n",
    "nllb_bpe_path = os.path.join(nllb_model_dir, 'spm.model')\n",
    "nllb_tokenizer = Tokenizer(nllb_bpe_path)\n",
    "\n",
    "def preprocess_nllb(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    source_lang and target_lang can be either an NLLB language code (e.g., 'deu_Latn'),\n",
    "    or a 2-letter language code (e.g., 'de'), in which case it is mapped automatically to the correct format\n",
    "    \"\"\"\n",
    "    source_lang = lang_code_mapping.get(source_lang, source_lang)\n",
    "    target_lang = lang_code_mapping.get(target_lang, target_lang)\n",
    "    source_line = nllb_tokenizer.tokenize(source_line)\n",
    "    target_line = nllb_tokenizer.tokenize(target_line)\n",
    "    source_line = f'<lang:{source_lang}> {source_line}'.strip()\n",
    "    target_prefix = f'<lang:{target_lang}>'\n",
    "    target_line = f'{target_prefix} {target_line}'.strip()\n",
    "    return source_line, target_line, target_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nllb_test_sets = {}\n",
    "\n",
    "for pair in 'en-fr', 'fr-en', 'de-fr':\n",
    "    src, tgt = pair.split('-')\n",
    "    path = os.path.join(data_dir, f'test.{pair}')\n",
    "    dataset = load_dataset(path, src, tgt, preprocess_nllb, max_size=500)\n",
    "    binarize(dataset, source_dict=nllb_dict, target_dict=nllb_dict, sort=False)\n",
    "    iterator = BatchIterator(dataset, src, tgt, batch_size=NLLB_BATCH_SIZE, max_len=NLLB_MAX_LEN, shuffle=False)\n",
    "    nllb_test_sets[pair] = iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = evaluate_model(nllb_model, *nllb_test_sets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pivot translation DE->EN->FR performs worse than direct translation DE->FR\n",
    "src = list(nllb_test_sets['de-fr'].data['source_data'])\n",
    "ref = list(nllb_test_sets['de-fr'].data['target_data'])\n",
    "hyp = pivot_translation(nllb_model, src, preprocess_nllb, 'deu_Latn', 'fra_Latn', pivot_lang='eng_Latn')\n",
    "import sacrebleu\n",
    "chrf = sacrebleu.corpus_chrf(hyp, [ref]).score\n",
    "print(f'de-fr (pivot): chrF={chrf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try translating to/from your native language\n",
    "\n",
    "Find the language code for your languages of interest on this page: https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short-hand for translating with the NLLB-200 model:\n",
    "def translate_nllb(sentence, source_lang='eng_Latn', target_lang='fra_Latn', plot_attention=False,\n",
    "                   max_len=NLLB_MAX_LEN):\n",
    "    translate(\n",
    "        nllb_model,\n",
    "        sentence,\n",
    "        preprocess_nllb,\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang,\n",
    "        google_translate=False,\n",
    "        plot_attention=plot_attention,\n",
    "        max_len=max_len,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate_nllb(\"Hello, how are you?\", target_lang='hat_Latn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt NLLB-200 to the Tatoeba domain\n",
    "\n",
    "NLLB-200 was trained on massive amounts of data crawled from the web, which makes it a very good generic model. However, the 600M version suffers from \"negative interference\", i.e., it lacks capacity to handle this many languages (the 3.3B version performs considerably better). Adapting it to a specific language direction can significantly improve its performance.\n",
    "Moreover, we can improve its performance on a specific domain (e.g., our Tatoeba data) by finetuning it on data from that domain (AKA \"domain adaptation\").\n",
    "\n",
    "Here we will train small adapters instead of finetuning the entire model, as this requires much less GPU memory and it is faster to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EN-FR training data\n",
    "src, tgt = 'en', 'fr'\n",
    "\n",
    "train_path = os.path.join(data_dir, f'train.{src}-{tgt}')\n",
    "valid_path = os.path.join(data_dir, f'valid.{src}-{tgt}')\n",
    "\n",
    "nllb_train_data = load_dataset(train_path, src, tgt, preprocess_nllb, max_size=10000)\n",
    "# set max_size to None to load the entire train set\n",
    "nllb_valid_data = load_dataset(valid_path, src, tgt, preprocess_nllb, max_size=500)\n",
    "\n",
    "binarize(nllb_train_data, source_dict=nllb_dict, target_dict=nllb_dict, sort=True)\n",
    "binarize(nllb_valid_data, source_dict=nllb_dict, target_dict=nllb_dict, sort=False)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "nllb_train_iterator = BatchIterator(\n",
    "    nllb_train_data, src, tgt, batch_size=NLLB_BATCH_SIZE, max_len=NLLB_MAX_LEN, shuffle=True\n",
    ")\n",
    "nllb_valid_iterator = BatchIterator(\n",
    "    nllb_valid_data, src, tgt, batch_size=NLLB_BATCH_SIZE, max_len=NLLB_MAX_LEN, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train adapters on EN-FR Tatoeba (domain adaptation)\n",
    "with nllb_model.adapter('en-fr', projection_dim=64, overwrite=True):\n",
    "    nllb_model.reset_optimizer()  # this must always be done when adding new adapters\n",
    "    train_model(nllb_model, nllb_train_iterator, [nllb_valid_iterator], checkpoint_path=None, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "print('# Original model')\n",
    "chrf = evaluate_model(nllb_model, nllb_test_sets['en-fr'])\n",
    "print()\n",
    "\n",
    "print('# Adapted model')\n",
    "with nllb_model.adapter('en-fr'):\n",
    "    chrf = evaluate_model(nllb_model, nllb_test_sets['en-fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness to noise\n",
    "\n",
    "Try reading the following sentence: ```The poet was sntiitg alone in his own ltlite room on a very sortmy evienng; the wind was rnoirag otiudse, and the rian puerod dwon in tntorers.```\n",
    "\n",
    "This is a noisy version of:\n",
    "```The poet was sitting alone in his own little room on a very stormy evening; the wind was roaring outside, and the rain poured down in torrents.``` where all letters but the first and the last in each word have been shuffled.\n",
    "\n",
    "Interestingly, it does not require us too much effort to parse this sort of text. Let's see how NLLB-200 fares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase maximum source and output length\n",
    "translate_nllb(\n",
    "    \"The poet was sntiitg alone in his own ltlite room on a very sortmy evienng; \"\n",
    "    \"the wind was rnoirag otiudse, and the rian puerod dwon in tntorers.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model doesn't completely break but the translation is rather bad (read [this paper](https://arxiv.org/abs/1711.02173) for more information about this phenomenon).\n",
    "Let's try to finetune NLLB-200 to improve its robustness to this specific type of noise.\n",
    "\n",
    "`permute_letters` below takes a sentence and shuffles letters in each word except the first and last letter.\n",
    "\n",
    "`preprocess_nllb_permute_letters` takes a pair of lines and shuffles the source side.\n",
    "\n",
    "The goal is to train the model with noisy sources and clean targets to make it invariant to this sort of noise (i.e., the clean and noisy versions of the same sentence should give the same output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_letters(sentence):\n",
    "    words = split_by_punct(sentence)\n",
    "    noised_words = []\n",
    "    for word in words:\n",
    "        if len(word) >= 3:\n",
    "            word = word[0] + ''.join(np.random.permutation(list(word[1:-1]))) + word[-1]\n",
    "        noised_words.append(word)\n",
    "    return ''.join(noised_words)\n",
    "\n",
    "def preprocess_nllb_permute_letters(source_line, target_line, source_lang=None, target_lang=None):\n",
    "    return preprocess_nllb(\n",
    "        permute_letters(source_line),\n",
    "        target_line,\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EN-FR training data and add noise to its source side\n",
    "src, tgt = 'en', 'fr'\n",
    "\n",
    "train_path = os.path.join(data_dir, f'train.{src}-{tgt}')\n",
    "valid_path = os.path.join(data_dir, f'valid.{src}-{tgt}')\n",
    "test_path = os.path.join(data_dir, f'test.{src}-{tgt}')\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_data_noisy = None   # to do\n",
    "valid_data_noisy = None   # to do\n",
    "test_data_noisy = None    # to do\n",
    "implemented = False       # set to True once you're done\n",
    "\n",
    "if implemented:\n",
    "    test_iterator_clean = nllb_test_sets['en-fr']\n",
    "\n",
    "    binarize(train_data_noisy, source_dict=nllb_dict, target_dict=nllb_dict, sort=True)\n",
    "    train_iterator_noisy = BatchIterator(\n",
    "        train_data_noisy, src, tgt, batch_size=NLLB_BATCH_SIZE, max_len=NLLB_MAX_LEN, shuffle=True\n",
    "    )\n",
    "    \n",
    "    binarize(valid_data_noisy, source_dict=nllb_dict, target_dict=nllb_dict, sort=False)\n",
    "    valid_iterator_noisy = BatchIterator(\n",
    "        valid_data_noisy, src, tgt, batch_size=NLLB_BATCH_SIZE, max_len=NLLB_MAX_LEN, shuffle=False\n",
    "    )\n",
    "\n",
    "    binarize(test_data_noisy, source_dict=nllb_dict, target_dict=nllb_dict, sort=False)\n",
    "    test_iterator_noisy = BatchIterator(\n",
    "        test_data_noisy, src, tgt, batch_size=NLLB_BATCH_SIZE, max_len=NLLB_MAX_LEN, shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if implemented:\n",
    "    chrf = evaluate_model(nllb_model, valid_iterator_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train adapters called `en-fr-noisy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: train noise adapters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate_nllb(\n",
    "    \"Someone ate all the cookies from the cookie jar.\",\n",
    ")\n",
    "\n",
    "translate_nllb(\n",
    "    \"Seoonme ate all the coieoks from the cikooe jar.\",\n",
    ")\n",
    "\n",
    "translate_nllb(\n",
    "    \"The poet was sntiitg alone in his own ltlite room on a very sortmy evienng; \"\n",
    "    \"the wind was rnoirag otiudse, and the rian puerod dwon in tntorers.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test your noise adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: test your noise adapters on the examples above and compute chrF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try other types of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(line):\n",
    "    return line\n",
    "def alphanum(line):\n",
    "    return re.sub(r'\\W', '', line)\n",
    "def capitalized(line):\n",
    "    return line.upper()\n",
    "def lowercase(line):\n",
    "    return line.lower()\n",
    "\n",
    "letters = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "def char_noise(line):\n",
    "    chars = list(line + ' ')\n",
    "    for i in range(len(chars)):\n",
    "        p = np.random.rand()\n",
    "        if p < 0.05:   # sub (5% prob)\n",
    "            chars[i] = np.random.choice(letters)\n",
    "        elif p < 0.05: # del (5% prob)\n",
    "            chars[i] = ''\n",
    "        elif p < 0.05: # ins (5% prob)\n",
    "            chars[i] = np.random.choice(letters) + chars[i]\n",
    "        else:          # nothing (85% prob)\n",
    "            pass\n",
    "    return ''.join(chars)\n",
    "\n",
    "src, tgt = 'en', 'fr'\n",
    "valid_path = os.path.join(data_dir, f'valid.{src}-{tgt}')\n",
    "\n",
    "for noise_fn in identity, permute_letters, alphanum, capitalized, lowercase, char_noise:\n",
    "    def preprocess_(source_line, *args, **kwargs):\n",
    "        return preprocess_nllb(noise_fn(source_line), *args, **kwargs)\n",
    "\n",
    "    reset_seed()\n",
    "    valid_data_ = load_dataset(valid_path, src, tgt, preprocess_, max_size=500)\n",
    "    binarize(valid_data_, source_dict=nllb_dict, target_dict=nllb_dict, sort=False)\n",
    "    valid_iterator_ = BatchIterator(\n",
    "        valid_data_, src, tgt, batch_size=NLLB_BATCH_SIZE, max_len=NLLB_MAX_LEN, shuffle=False\n",
    "    )\n",
    "    \n",
    "    print('#', noise_fn.__name__)\n",
    "    example = noise_fn(\"Someone ate all the cookies from the cookie jar.\")\n",
    "    translation = get_translations(nllb_model, [example], preprocess_nllb, src, tgt)['predictions_detok'][0]\n",
    "    print(f'Example: \"{example}\" -> \"{translation}\"')\n",
    "    chrf = evaluate_model(nllb_model, valid_iterator_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out the other topics in MT!\n",
    "There are also many other interesting and important research topics in MT which are not covered in this lab session.\n",
    "\n",
    "Here are some of them:\n",
    "- **Unsupervised or low-resource MT**\n",
    "  - How can we leverage monolingual data when bilingual data is not available or extremely scarce?\n",
    "  - How can we improve the performance of low-resource language pairs?\n",
    "- **Document-level context-aware MT**\n",
    "  - How can we effectively translate a text containing multiple sentences while keeping the translation coherent and faithful?\n",
    "- **Domain-adapted or personalized MT, continual learning for MT**\n",
    "  - How can we extend an existing model for new domains, language pairs, or simply new addition of data?\n",
    "- **Efficient MT**\n",
    "  - How can we train and serve MT models more efficiently (both in terms of memory usage and CPU/GPU computation)\n",
    "  \n",
    "If you are interested in finding out more about MT, you can check out the [WMT conference](https://www.statmt.org/wmt22/) that is held annually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b9508ad9fde0fbdec605f55d97fdf78e9458d48dc88ea45cb3b3558c0b97694"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
